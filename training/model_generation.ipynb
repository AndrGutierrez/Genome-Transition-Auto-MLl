{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-11T15:10:25.156741Z",
     "start_time": "2025-04-11T15:10:20.145018Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T15:10:26.595120Z",
     "start_time": "2025-04-11T15:10:26.574004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove models folder if it exists\n",
    "import shutil\n",
    "shutil.rmtree(\"../models\", ignore_errors=True)"
   ],
   "id": "6c0e6a43ad138192",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T15:10:29.155531Z",
     "start_time": "2025-04-11T15:10:29.139644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(true_data, counter_files, max_index):\n",
    "    combined_data = {}\n",
    "    # Define the column names to keep.\n",
    "    cols = [f\"B{i}\" for i in range(1, max_index + 1)]\n",
    "\n",
    "    # Process the true data: retain only the B columns and add the true label.\n",
    "    true_data['label'] = \"true\"\n",
    "    true_data = true_data[cols + [\"label\"]]\n",
    "\n",
    "    for key, df_counter in counter_files.items():\n",
    "        # Retain only the B columns and add the false label.\n",
    "        df_counter['label'] = \"false\"\n",
    "        df_counter = df_counter[cols + [\"label\"]]\n",
    "        # Combine the true data with this counter example.\n",
    "        combined = pd.concat([true_data, df_counter], ignore_index=True)\n",
    "        combined_data[key] = combined\n",
    "\n",
    "    return combined_data"
   ],
   "id": "1ba82cefa1f193c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T15:10:31.153362Z",
     "start_time": "2025-04-11T15:10:31.127125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "presets = ['good_quality', 'optimize_for_deployment']\n",
    "\n",
    "def generateModels(combined_data, prefix, max_index, time_limit=180):\n",
    "    results = {}\n",
    "    # Define the feature columns\n",
    "    feature_cols = [f\"B{i}\" for i in range(1, max_index + 1)]\n",
    "\n",
    "    for key, df in combined_data.items():\n",
    "        # Split data into features and label.\n",
    "        X = df[feature_cols]\n",
    "        y = df[\"label\"]\n",
    "\n",
    "        # Train-test split: 70% train, 30% test.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        train_data = pd.concat([X_train, y_train], axis=1)\n",
    "        test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        # Define an output folder for saving models for this dataset variant.\n",
    "        output_folder = f\"../models/{prefix}/{key}\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Train the model using AutoGluon.\n",
    "        predictor = TabularPredictor(label=\"label\", path=output_folder, eval_metric='f1').fit(train_data, presets=presets, time_limit=time_limit)\n",
    "\n",
    "        # Evaluate the model on the test data.\n",
    "        leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "        results[key] = leaderboard\n",
    "\n",
    "        print(f\"Results for {key}:\")\n",
    "        print(leaderboard)\n",
    "\n",
    "    # Create results folder and save leaderboards.\n",
    "    results_folder = f\"../models/{prefix}/results\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    for key, result in results.items():\n",
    "        result.to_csv(os.path.join(results_folder, f\"{key}_leaderboard.csv\"), index=False)"
   ],
   "id": "81f67c0ce624347c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:14:20.291078Z",
     "start_time": "2025-03-28T15:30:42.156225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_file = \"../data/ei/data_ei.csv\"\n",
    "ei_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.read_csv(\"../data/ei/data_ei_negative_sample.csv\")\n",
    "}\n",
    "\n",
    "# For EI zone, our CSV files have columns B1 to B12.\n",
    "combined_data = prepare_data(ei_true_data, counter_files, max_index=12)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ei', 12, time_limit=21600)"
   ],
   "id": "2d84d45e4f7d2f32",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../models/ei/combined\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       6.45 GB / 15.35 GB (42.0%)\n",
      "Disk Space Avail:   47.75 GB / 100.00 GB (47.7%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality', 'optimize_for_deployment']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5400s of the 21600s of remaining time (25%).\n",
      "2025-03-28 11:30:43,338\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-03-28 11:30:48,446\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n",
      "\t\tContext path: \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Running DyStack sub-fit ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Beginning AutoGluon training ... Time limit = 5390s\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Train Data Rows:    85576\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Train Data Columns: 12\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Label Column:       label\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Problem Type:       binary\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Preprocessing data ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Using Feature Generators to preprocess the data ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tAvailable Memory:                    5990.04 MB\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTrain Data (Original)  Memory Usage: 56.80 MB (0.9% of available memory)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 1 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 2 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 3 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 4 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 5 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\t('object', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\t('category', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.4s = Fit runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t12 features in original data used to generate 12 features in processed data.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTrain Data (Processed) Memory Usage: 0.99 MB (0.0% of available memory)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Data preprocessing and feature engineering runtime = 0.4s ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m User-specified model hyperparameters to be fit:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m {\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'NN_TORCH': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'CAT': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'XGB': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'FASTAI': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m }\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3592.32s of the 5389.83s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.063705\tvalid_set's f1: 0.957524\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [2000]\tvalid_set's binary_logloss: 0.0596066\tvalid_set's f1: 0.962963\u001B[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [3000]\tvalid_set's binary_logloss: 0.058624\tvalid_set's f1: 0.963412\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [4000]\tvalid_set's binary_logloss: 0.0589464\tvalid_set's f1: 0.965462\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [5000]\tvalid_set's binary_logloss: 0.0598116\tvalid_set's f1: 0.965489\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [6000]\tvalid_set's binary_logloss: 0.0608523\tvalid_set's f1: 0.966505\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [7000]\tvalid_set's binary_logloss: 0.0619875\tvalid_set's f1: 0.966714\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [8000]\tvalid_set's binary_logloss: 0.0635435\tvalid_set's f1: 0.966895\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [9000]\tvalid_set's binary_logloss: 0.06511\tvalid_set's f1: 0.967286\u001B[32m [repeated 4x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [10000]\tvalid_set's binary_logloss: 0.0663259\tvalid_set's f1: 0.966882\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9675\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t132.8s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t51.79s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 3442.75s of the 5240.25s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.0611877\tvalid_set's f1: 0.960582\n",
      "\u001B[36m(_ray_fit pid=24548)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.0657267\tvalid_set's f1: 0.955569\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [2000]\tvalid_set's binary_logloss: 0.0575243\tvalid_set's f1: 0.965253\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [3000]\tvalid_set's binary_logloss: 0.0570503\tvalid_set's f1: 0.96631\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [4000]\tvalid_set's binary_logloss: 0.0580491\tvalid_set's f1: 0.96713\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [5000]\tvalid_set's binary_logloss: 0.059545\tvalid_set's f1: 0.966895\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=20476)\u001B[0m [6000]\tvalid_set's binary_logloss: 0.0584109\tvalid_set's f1: 0.966465\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [7000]\tvalid_set's binary_logloss: 0.061186\tvalid_set's f1: 0.969855\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [8000]\tvalid_set's binary_logloss: 0.0626911\tvalid_set's f1: 0.970636\u001B[32m [repeated 4x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [9000]\tvalid_set's binary_logloss: 0.0641141\tvalid_set's f1: 0.970233\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [10000]\tvalid_set's binary_logloss: 0.0655922\tvalid_set's f1: 0.970417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9675\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t135.31s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t45.56s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 3294.71s of the 5092.22s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9627\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.53s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.67s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3290.31s of the 5087.81s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9614\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.03s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.68s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 3286.40s of the 5083.90s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n",
      "\u001B[36m(_ray_fit pid=22184)\u001B[0m \tRan out of time, early stopping on iteration 8022.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9601\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2629.87s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.62s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 653.29s of the 2450.80s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9584\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.18s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 649.08s of the 2446.59s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=24292)\u001B[0m \tRan out of time, early stopping on iteration 8191.\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9569\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.72s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.7s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 645.48s of the 2442.98s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9581\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t124.3s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.45s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 511.13s of the 2308.64s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.30%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9679\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t110.17s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t3.01s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 397.34s of the 2194.84s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.16%)\n",
      "\u001B[36m(_ray_fit pid=23048)\u001B[0m \tRan out of time, stopping training early. (Stopping on epoch 77)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9673\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t319.95s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.5s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 74.17s of the 1871.68s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.41%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.0566452\tvalid_set's f1: 0.962888\n",
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m [2000]\tvalid_set's binary_logloss: 0.0558062\tvalid_set's f1: 0.965337\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m [3000]\tvalid_set's binary_logloss: 0.0568761\tvalid_set's f1: 0.967104\u001B[32m [repeated 7x across cluster]\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m \tRan out of time, early stopping on iteration 3371. Best iteration is:\n",
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m \t[3366]\tvalid_set's binary_logloss: 0.0576004\tvalid_set's f1: 0.968094\n",
      "\u001B[36m(_ray_fit pid=23988)\u001B[0m \tRan out of time, stopping training early. (Stopping on epoch 78)\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9685\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t61.51s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t107.11s\t = Validation runtime\n",
      "\u001B[36m(_ray_fit pid=13232)\u001B[0m \tRan out of time, early stopping on iteration 3303. Best iteration is:\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=13232)\u001B[0m \t[2885]\tvalid_set's binary_logloss: 0.0579046\tvalid_set's f1: 0.965852\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1790.53s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.3, 'LightGBMXT_BAG_L1': 0.15, 'LightGBM_BAG_L1': 0.15, 'RandomForestGini_BAG_L1': 0.15, 'RandomForestEntr_BAG_L1': 0.1, 'ExtraTreesGini_BAG_L1': 0.1, 'LightGBMLarge_BAG_L1': 0.05}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9702\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.04s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.02s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1785.43s of the 1785.42s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.58%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9704\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t7.73s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.59s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1774.40s of the 1774.39s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.58%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9717\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t4.11s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.17s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1767.09s of the 1767.08s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9725\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.66s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.69s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1759.55s of the 1759.54s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9721\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.24s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.64s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1752.48s of the 1752.48s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.64%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9718\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t352.46s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.14s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 1396.89s of the 1396.88s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9726\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.71s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 1392.27s of the 1392.26s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.972\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.3s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.71s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1388.08s of the 1388.07s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.86%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9725\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t133.67s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.06s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 1251.13s of the 1251.12s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.86%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9716\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t10.47s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.35s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1237.36s of the 1237.36s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.50%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9692\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t326.83s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.71s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 907.24s of the 907.23s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.80%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9727\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t10.82s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.62s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 892.94s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'LightGBMLarge_BAG_L2': 0.75, 'ExtraTreesGini_BAG_L2': 0.167, 'RandomForestGini_BAG_L1': 0.083}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9727\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t9.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.02s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon training complete, total runtime = 4506.94s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 50.5 rows/s (10697 batch size)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t8.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t8.61s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.53s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.67s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.03s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.68s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t601.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.18s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.72s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.7s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m No improvement since epoch 0: early stopping\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t49.28s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t3.65s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t161.67s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t8.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.3, 'LightGBMXT_BAG_L1': 0.15, 'LightGBM_BAG_L1': 0.15, 'RandomForestGini_BAG_L1': 0.15, 'RandomForestEntr_BAG_L1': 0.1, 'ExtraTreesGini_BAG_L1': 0.1, 'LightGBMLarge_BAG_L1': 0.05}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.04s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.74s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.44s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.66s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.69s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.24s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.64s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t68.34s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.71s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.3s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.71s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m No improvement since epoch 0: early stopping\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t54.0s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t57.61s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.02s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'LightGBMLarge_BAG_L2': 0.75, 'ExtraTreesGini_BAG_L2': 0.167, 'RandomForestGini_BAG_L1': 0.083}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t9.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Refit complete, total runtime = 1030.71s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tBase Threshold: 0.500\t| val: 0.9727\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tBest Threshold: 0.502\t| val: 0.9727\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Updating predictor.decision_threshold from 0.5 -> 0.502\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tPrediction probabilities of the positive class >0.502 will be predicted as the positive class (true). This can significantly impact metric scores.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    NeuralNetFastAI_BAG_L2_FULL       0.972525   0.972518          f1        3.229756            NaN  905.557942                 0.159154                     NaN          53.996839            2       True         20\n",
      "1     ExtraTreesGini_BAG_L2_FULL       0.972362   0.972582          f1        3.209129            NaN  854.268252                 0.138527                1.725116           2.707150            2       True         18\n",
      "2           CatBoost_BAG_L2_FULL       0.972166   0.971815          f1        3.091247            NaN  919.904276                 0.020645                     NaN          68.343173            2       True         17\n",
      "3       WeightedEnsemble_L3_FULL       0.972166   0.972695          f1        3.272931            NaN  864.904662                 0.015032                     NaN           9.616641            3       True         24\n",
      "4     ExtraTreesEntr_BAG_L2_FULL       0.971797   0.971995          f1        3.216191            NaN  853.862036                 0.145589                1.706968           2.300934            2       True         19\n",
      "5            XGBoost_BAG_L2_FULL       0.971751   0.971565          f1        3.130604            NaN  852.179468                 0.060002                     NaN           0.618366            2       True         21\n",
      "6   RandomForestGini_BAG_L2_FULL       0.971578   0.972525          f1        3.218326            NaN  857.225250                 0.147724                1.690533           5.664147            2       True         15\n",
      "7         LightGBMXT_BAG_L2_FULL       0.971383   0.970391          f1        3.102315            NaN  852.296472                 0.031713                     NaN           0.735370            2       True         13\n",
      "8      LightGBMLarge_BAG_L2_FULL       0.971359   0.972662          f1        3.119372            NaN  852.580871                 0.048770                     NaN           1.019769            2       True         23\n",
      "9   RandomForestEntr_BAG_L2_FULL       0.971175   0.972124          f1        3.197128            NaN  856.801207                 0.126526                1.640430           5.240104            2       True         16\n",
      "10          LightGBM_BAG_L2_FULL       0.971129   0.971657          f1        3.095801            NaN  852.001240                 0.025199                     NaN           0.440137            2       True         14\n",
      "11          LightGBM_BAG_L1_FULL       0.970257   0.967521          f1        0.601471            NaN    8.614685                 0.601471                     NaN           8.614685            1       True          2\n",
      "12      WeightedEnsemble_L2_FULL       0.969453   0.970176          f1        2.587145            NaN  199.989231                 0.015180                     NaN           5.041221            2       True         12\n",
      "13     LightGBMLarge_BAG_L1_FULL       0.969246   0.968470          f1        0.679222            NaN    8.957639                 0.679222                     NaN           8.957639            1       True         11\n",
      "14    NeuralNetTorch_BAG_L2_FULL       0.968029   0.969195          f1        3.177464            NaN  909.172722                 0.106863                     NaN          57.611620            2       True         22\n",
      "15        LightGBMXT_BAG_L1_FULL       0.967884   0.967485          f1        0.698297            NaN    8.962849                 0.698297                     NaN           8.962849            1       True          1\n",
      "16    NeuralNetTorch_BAG_L1_FULL       0.966319   0.967343          f1        0.076677            NaN  161.669907                 0.076677                     NaN         161.669907            1       True         10\n",
      "17           XGBoost_BAG_L1_FULL       0.965738   0.967870          f1        0.133964            NaN    3.653278                 0.133964                     NaN           3.653278            1       True          9\n",
      "18  RandomForestGini_BAG_L1_FULL       0.963366   0.962727          f1        0.185536       1.666057    2.534030                 0.185536                1.666057           2.534030            1       True          3\n",
      "19  RandomForestEntr_BAG_L1_FULL       0.962336   0.961428          f1        0.160317       1.676287    2.032543                 0.160317                1.676287           2.032543            1       True          4\n",
      "20    ExtraTreesGini_BAG_L1_FULL       0.959258   0.958381          f1        0.170444       1.729316    2.176358                 0.170444                1.729316           2.176358            1       True          6\n",
      "21          CatBoost_BAG_L1_FULL       0.959064   0.960083          f1        0.058254            NaN  601.956029                 0.058254                     NaN         601.956029            1       True          5\n",
      "22    ExtraTreesEntr_BAG_L1_FULL       0.956837   0.956938          f1        0.154971       1.695606    1.718949                 0.154971                1.695606           1.718949            1       True          7\n",
      "23   NeuralNetFastAI_BAG_L1_FULL       0.924867   0.958110          f1        0.151449            NaN   49.284837                 0.151449                     NaN          49.284837            1       True          8\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t5556s\t = DyStack   runtime |\t16044s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 16044s\n",
      "AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\"\n",
      "Train Data Rows:    96273\n",
      "Train Data Columns: 12\n",
      "Label Column:       label\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6404.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.90 MB (1.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.11 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 10692.98s of the 16043.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.22%)\n",
      "\t0.9697\t = Validation score   (f1)\n",
      "\t151.83s\t = Training   runtime\n",
      "\t112.99s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 10519.72s of the 15870.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.9646\t = Validation score   (f1)\n",
      "\t121.24s\t = Training   runtime\n",
      "\t31.69s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 10385.39s of the 15735.90s of remaining time.\n",
      "\t0.9637\t = Validation score   (f1)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t1.79s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 10380.50s of the 15731.00s of remaining time.\n",
      "\t0.9621\t = Validation score   (f1)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 10376.01s of the 15726.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.26%)\n",
      "\t0.9627\t = Validation score   (f1)\n",
      "\t3625.98s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 6746.70s of the 12097.21s of remaining time.\n",
      "\t0.9594\t = Validation score   (f1)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t1.86s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 6742.37s of the 12092.88s of remaining time.\n",
      "\t0.957\t = Validation score   (f1)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t1.85s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 6738.21s of the 12088.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.34%)\n",
      "\t0.9584\t = Validation score   (f1)\n",
      "\t138.82s\t = Training   runtime\n",
      "\t1.56s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 6595.96s of the 11946.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.48%)\n",
      "\t0.9697\t = Validation score   (f1)\n",
      "\t170.55s\t = Training   runtime\n",
      "\t3.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 6421.18s of the 11771.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.19%)\n",
      "\t0.9711\t = Validation score   (f1)\n",
      "\t1260.35s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5157.52s of the 10508.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.36%)\n",
      "\t0.9711\t = Validation score   (f1)\n",
      "\t103.18s\t = Training   runtime\n",
      "\t81.83s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1069.30s of the 10395.69s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.375, 'RandomForestGini_BAG_L1': 0.25, 'RandomForestEntr_BAG_L1': 0.125, 'XGBoost_BAG_L1': 0.125, 'LightGBMLarge_BAG_L1': 0.125}\n",
      "\t0.9726\t = Validation score   (f1)\n",
      "\t5.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 10390.14s of the 10390.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.68%)\n",
      "\t0.9726\t = Validation score   (f1)\n",
      "\t7.51s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 10379.29s of the 10379.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.69%)\n",
      "\t0.9737\t = Validation score   (f1)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 10372.82s of the 10372.80s of remaining time.\n",
      "\t0.9747\t = Validation score   (f1)\n",
      "\t6.55s\t = Training   runtime\n",
      "\t1.76s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 10364.32s of the 10364.30s of remaining time.\n",
      "\t0.9751\t = Validation score   (f1)\n",
      "\t5.83s\t = Training   runtime\n",
      "\t1.74s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 10356.57s of the 10356.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.75%)\n",
      "\t0.9739\t = Validation score   (f1)\n",
      "\t325.77s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 10027.55s of the 10027.54s of remaining time.\n",
      "\t0.9745\t = Validation score   (f1)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t1.87s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 10022.33s of the 10022.31s of remaining time.\n",
      "\t0.9746\t = Validation score   (f1)\n",
      "\t2.81s\t = Training   runtime\n",
      "\t1.88s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 10017.45s of the 10017.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.05%)\n",
      "\t0.9746\t = Validation score   (f1)\n",
      "\t144.44s\t = Training   runtime\n",
      "\t1.54s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 9869.67s of the 9869.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.09%)\n",
      "\t0.9742\t = Validation score   (f1)\n",
      "\t10.29s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 9855.90s of the 9855.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.62%)\n",
      "\t0.9715\t = Validation score   (f1)\n",
      "\t385.96s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 9466.54s of the 9466.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.93%)\n",
      "\t0.9741\t = Validation score   (f1)\n",
      "\t5.13s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 1039.01s of the 9457.81s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestEntr_BAG_L2': 1.0}\n",
      "\t0.9751\t = Validation score   (f1)\n",
      "\t10.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6597.0s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 51.5 rows/s (12035 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t12.34s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t7.02s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.89s\t = Training   runtime\n",
      "\t1.79s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.5s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t808.88s\t = Training   runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.25s\t = Training   runtime\n",
      "\t1.86s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.1s\t = Training   runtime\n",
      "\t1.85s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "No improvement since epoch 0: early stopping\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\t55.54s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t4.3s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t349.51s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t11.96s\t = Training   runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t5.83s\t = Training   runtime\n",
      "\t1.74s\t = Validation runtime\n",
      "Updated best model to \"RandomForestEntr_BAG_L2_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"RandomForestEntr_BAG_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 1252.53s ... Best model: \"RandomForestEntr_BAG_L2_FULL\"\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.9751\n",
      "\tBest Threshold: 0.559\t| val: 0.9752\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.559\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.559 will be predicted as the positive class (true). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "Deleting model LightGBMXT_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMXT_BAG_L1 will be removed.\n",
      "Deleting model LightGBM_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBM_BAG_L1 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestGini_BAG_L1 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestEntr_BAG_L1 will be removed.\n",
      "Deleting model CatBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\CatBoost_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesGini_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesEntr_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetFastAI_BAG_L1 will be removed.\n",
      "Deleting model XGBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\XGBoost_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetTorch_BAG_L1 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMLarge_BAG_L1 will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\WeightedEnsemble_L2 will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMXT_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBM_BAG_L2 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestGini_BAG_L2 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestEntr_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\CatBoost_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesGini_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesEntr_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetFastAI_BAG_L2 will be removed.\n",
      "Deleting model XGBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\XGBoost_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetTorch_BAG_L2 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMLarge_BAG_L2 will be removed.\n",
      "Deleting model WeightedEnsemble_L3. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\WeightedEnsemble_L3 will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\")\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for combined:\n",
      "                           model  score_test score_val eval_metric  \\\n",
      "0   RandomForestEntr_BAG_L2_FULL    0.973865      None          f1   \n",
      "1      LightGBMLarge_BAG_L1_FULL    0.972153      None          f1   \n",
      "2            XGBoost_BAG_L1_FULL    0.971640      None          f1   \n",
      "3         LightGBMXT_BAG_L1_FULL    0.970114      None          f1   \n",
      "4           LightGBM_BAG_L1_FULL    0.969751      None          f1   \n",
      "5     NeuralNetTorch_BAG_L1_FULL    0.968885      None          f1   \n",
      "6   RandomForestGini_BAG_L1_FULL    0.964867      None          f1   \n",
      "7           CatBoost_BAG_L1_FULL    0.964247      None          f1   \n",
      "8   RandomForestEntr_BAG_L1_FULL    0.962901      None          f1   \n",
      "9     ExtraTreesGini_BAG_L1_FULL    0.956632      None          f1   \n",
      "10    ExtraTreesEntr_BAG_L1_FULL    0.954728      None          f1   \n",
      "11   NeuralNetFastAI_BAG_L1_FULL    0.915713      None          f1   \n",
      "\n",
      "    pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
      "0         9.866940            NaN  1265.118521                 0.185287   \n",
      "1         2.829647            NaN    11.962454                 2.829647   \n",
      "2         0.414227            NaN     4.297555                 0.414227   \n",
      "3         3.666034            NaN    12.339880                 3.666034   \n",
      "4         1.176956            NaN     7.021670                 1.176956   \n",
      "5         0.176993            NaN   349.512278                 0.176993   \n",
      "6         0.234456       1.786993     2.886807                 0.234456   \n",
      "7         0.117523            NaN   808.883289                 0.117523   \n",
      "8         0.202614       1.770829     2.495834                 0.202614   \n",
      "9         0.202652       1.864680     2.254139                 0.202652   \n",
      "10        0.202750       1.848873     2.099149                 0.202750   \n",
      "11        0.457801            NaN    55.538630                 0.457801   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                 1.737098           5.826838            2       True   \n",
      "1                      NaN          11.962454            1       True   \n",
      "2                      NaN           4.297555            1       True   \n",
      "3                      NaN          12.339880            1       True   \n",
      "4                      NaN           7.021670            1       True   \n",
      "5                      NaN         349.512278            1       True   \n",
      "6                 1.786993           2.886807            1       True   \n",
      "7                      NaN         808.883289            1       True   \n",
      "8                 1.770829           2.495834            1       True   \n",
      "9                 1.864680           2.254139            1       True   \n",
      "10                1.848873           2.099149            1       True   \n",
      "11                     NaN          55.538630            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0          12  \n",
      "1          11  \n",
      "2           9  \n",
      "3           1  \n",
      "4           2  \n",
      "5          10  \n",
      "6           3  \n",
      "7           5  \n",
      "8           4  \n",
      "9           6  \n",
      "10          7  \n",
      "11          8  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T04:53:08.333525Z",
     "start_time": "2025-04-11T22:15:37.266534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_file = \"../data/ie/data_ie.csv\"\n",
    "ie_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.read_csv(\"../data/ie/data_ie_negative_sample.csv\")\n",
    "}\n",
    "\n",
    "# For EI zone, our CSV files have columns B1 to B105.\n",
    "combined_data = prepare_data(ie_true_data, counter_files, max_index=105)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ie', 105, time_limit=21000)"
   ],
   "id": "16ef630f027dbbdb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\AppData\\Local\\Temp\\ipykernel_29112\\3070329821.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ie_true_data = pd.read_csv(true_file)\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../models/ie/combined\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       5.91 GB / 15.35 GB (38.5%)\n",
      "Disk Space Avail:   44.11 GB / 100.00 GB (44.1%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality', 'optimize_for_deployment']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5250s of the 21000s of remaining time (25%).\n",
      "\t\tContext path: \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L3_FULL       0.986763   0.984266          f1        8.623333            NaN  1041.443308                 0.023818                     NaN          34.021181            3       True         20\n",
      "1   RandomForestEntr_BAG_L2_FULL       0.986357   0.984091          f1        6.124680            NaN   848.398977                 0.325795               11.290100          18.923876            2       True         14\n",
      "2           CatBoost_BAG_L2_FULL       0.986346   0.983416          f1        5.928632            NaN  1153.597274                 0.129747                     NaN         324.122172            2       True         15\n",
      "3         LightGBMXT_BAG_L2_FULL       0.986344   0.983693          f1        5.885725            NaN   833.552903                 0.086841                     NaN           4.077802            2       True         11\n",
      "4    NeuralNetFastAI_BAG_L2_FULL       0.986282   0.983998          f1        7.045536            NaN   939.501726                 1.246651                     NaN         110.026625            2       True         18\n",
      "5   RandomForestGini_BAG_L2_FULL       0.986244   0.983947          f1        6.113984            NaN   851.655190                 0.315099               11.553575          22.180088            2       True         13\n",
      "6            XGBoost_BAG_L2_FULL       0.986053   0.983696          f1        6.285498            NaN   838.218518                 0.486613                     NaN           8.743417            2       True         19\n",
      "7           LightGBM_BAG_L2_FULL       0.985986   0.983647          f1        5.876216            NaN   831.938655                 0.077331                     NaN           2.463554            2       True         12\n",
      "8     ExtraTreesEntr_BAG_L2_FULL       0.985811   0.983689          f1        6.131938            NaN   843.446911                 0.333054               11.286693          13.971809            2       True         17\n",
      "9     ExtraTreesGini_BAG_L2_FULL       0.985638   0.983738          f1        6.138517            NaN   843.470319                 0.339632               12.029153          13.995218            2       True         16\n",
      "10        LightGBMXT_BAG_L1_FULL       0.982606   0.980856          f1        0.995354            NaN    41.701812                 0.995354                     NaN          41.701812            1       True          1\n",
      "11          LightGBM_BAG_L1_FULL       0.982216   0.981200          f1        1.109219            NaN    50.287998                 1.109219                     NaN          50.287998            1       True          2\n",
      "12      WeightedEnsemble_L2_FULL       0.982216   0.981200          f1        1.113231            NaN    67.879131                 0.004012                     NaN          17.591133            2       True         10\n",
      "13   NeuralNetFastAI_BAG_L1_FULL       0.975873   0.975298          f1        1.257219            NaN   184.820119                 1.257219                     NaN         184.820119            1       True          8\n",
      "14    ExtraTreesGini_BAG_L1_FULL       0.975783   0.973229          f1        0.428474      12.608024    17.230587                 0.428474               12.608024          17.230587            1       True          6\n",
      "15  RandomForestGini_BAG_L1_FULL       0.974987   0.973116          f1        0.584494      12.618942    20.578415                 0.584494               12.618942          20.578415            1       True          3\n",
      "16    ExtraTreesEntr_BAG_L1_FULL       0.974784   0.972543          f1        0.413624      12.654063    16.691949                 0.413624               12.654063          16.691949            1       True          7\n",
      "17  RandomForestEntr_BAG_L1_FULL       0.973532   0.972104          f1        0.429102      12.378791    19.496723                 0.429102               12.378791          19.496723            1       True          4\n",
      "18          CatBoost_BAG_L1_FULL       0.961470   0.959496          f1        0.096937            NaN   468.912903                 0.096937                     NaN         468.912903            1       True          5\n",
      "19           XGBoost_BAG_L1_FULL       0.959629   0.955508          f1        0.484461            NaN     9.754596                 0.484461                     NaN           9.754596            1       True          9\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t6517s\t = DyStack   runtime |\t14483s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 14483s\n",
      "AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\"\n",
      "Train Data Rows:    154693\n",
      "Train Data Columns: 105\n",
      "Label Column:       label\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    8668.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 898.44 MB (10.4% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 10.4% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', []) : 105 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 105 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\t12.7s = Fit runtime\n",
      "\t105 features in original data used to generate 105 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 15.54 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 13.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 9643.89s of the 14469.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.59%)\n",
      "\t0.9815\t = Validation score   (f1)\n",
      "\t485.72s\t = Training   runtime\n",
      "\t61.29s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 9139.65s of the 13965.22s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.62%)\n",
      "\t0.9816\t = Validation score   (f1)\n",
      "\t698.89s\t = Training   runtime\n",
      "\t68.24s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 8412.48s of the 13238.05s of remaining time.\n",
      "\t0.974\t = Validation score   (f1)\n",
      "\t22.79s\t = Training   runtime\n",
      "\t14.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 8374.65s of the 13200.21s of remaining time.\n",
      "\t0.9733\t = Validation score   (f1)\n",
      "\t22.29s\t = Training   runtime\n",
      "\t14.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 8337.34s of the 13162.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.96%)\n",
      "\t0.9721\t = Validation score   (f1)\n",
      "\t6662.86s\t = Training   runtime\n",
      "\t1.22s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1667.35s of the 6492.91s of remaining time.\n",
      "\t0.9746\t = Validation score   (f1)\n",
      "\t19.47s\t = Training   runtime\n",
      "\t14.69s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1632.11s of the 6457.67s of remaining time.\n",
      "\t0.9733\t = Validation score   (f1)\n",
      "\t19.17s\t = Training   runtime\n",
      "\t14.55s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1597.39s of the 6422.96s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.57%)\n",
      "\t0.98\t = Validation score   (f1)\n",
      "\t1087.71s\t = Training   runtime\n",
      "\t12.93s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 500.54s of the 5326.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.55%)\n",
      "\t0.9671\t = Validation score   (f1)\n",
      "\t401.23s\t = Training   runtime\n",
      "\t7.51s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 91.42s of the 4916.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.25%)\n",
      "\t0.9616\t = Validation score   (f1)\n",
      "\t75.32s\t = Training   runtime\n",
      "\t9.39s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 7.49s of the 4833.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.09%)\n",
      "\t0.9514\t = Validation score   (f1)\n",
      "\t6.83s\t = Training   runtime\n",
      "\t1.09s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 964.39s of the 4818.87s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.316, 'LightGBMXT_BAG_L1': 0.263, 'LightGBM_BAG_L1': 0.158, 'ExtraTreesGini_BAG_L1': 0.105, 'ExtraTreesEntr_BAG_L1': 0.105, 'RandomForestGini_BAG_L1': 0.053}\n",
      "\t0.9819\t = Validation score   (f1)\n",
      "\t25.13s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4793.59s of the 4793.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.20%)\n",
      "\t0.9844\t = Validation score   (f1)\n",
      "\t42.13s\t = Training   runtime\n",
      "\t1.51s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4744.47s of the 4744.40s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.24%)\n",
      "\t0.9845\t = Validation score   (f1)\n",
      "\t13.82s\t = Training   runtime\n",
      "\t0.87s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 4723.59s of the 4723.53s of remaining time.\n",
      "\t0.9851\t = Validation score   (f1)\n",
      "\t27.27s\t = Training   runtime\n",
      "\t13.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 4682.10s of the 4682.03s of remaining time.\n",
      "\t0.9852\t = Validation score   (f1)\n",
      "\t22.09s\t = Training   runtime\n",
      "\t13.0s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4646.22s of the 4646.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.58%)\n",
      "\t0.9847\t = Validation score   (f1)\n",
      "\t3711.32s\t = Training   runtime\n",
      "\t1.37s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 928.18s of the 928.11s of remaining time.\n",
      "\t0.9848\t = Validation score   (f1)\n",
      "\t16.73s\t = Training   runtime\n",
      "\t13.68s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 896.65s of the 896.59s of remaining time.\n",
      "\t0.9848\t = Validation score   (f1)\n",
      "\t15.81s\t = Training   runtime\n",
      "\t13.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 866.55s of the 866.48s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.91%)\n",
      "\t0.9851\t = Validation score   (f1)\n",
      "\t602.24s\t = Training   runtime\n",
      "\t13.24s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 241.54s of the 241.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.95%)\n",
      "\t0.9846\t = Validation score   (f1)\n",
      "\t117.64s\t = Training   runtime\n",
      "\t5.16s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 116.27s of the 116.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.69%)\n",
      "\t0.9837\t = Validation score   (f1)\n",
      "\t99.06s\t = Training   runtime\n",
      "\t10.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 479.36s of the 7.44s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.5, 'RandomForestEntr_BAG_L2': 0.273, 'RandomForestGini_BAG_L2': 0.136, 'ExtraTreesGini_BAG_L1': 0.045, 'ExtraTreesEntr_BAG_L1': 0.045}\n",
      "\t0.9855\t = Validation score   (f1)\n",
      "\t46.46s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 14522.51s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 104.3 rows/s (19337 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t19.47s\t = Training   runtime\n",
      "\t14.69s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t19.17s\t = Training   runtime\n",
      "\t14.55s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t53.9s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t54.48s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t22.79s\t = Training   runtime\n",
      "\t14.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t22.29s\t = Training   runtime\n",
      "\t14.07s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t1805.0s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t622.01s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t13.94s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t52.04s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t2.52s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t27.27s\t = Training   runtime\n",
      "\t13.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t22.09s\t = Training   runtime\n",
      "\t13.0s\t = Validation runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\tStopping at the best epoch learned earlier - 4.\n",
      "\t153.29s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.5, 'RandomForestEntr_BAG_L2': 0.273, 'RandomForestGini_BAG_L2': 0.136, 'ExtraTreesGini_BAG_L1': 0.045, 'ExtraTreesEntr_BAG_L1': 0.045}\n",
      "\t46.46s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 2765.86s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.9855\n",
      "\tBest Threshold: 0.499\t| val: 0.9855\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.499\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.499 will be predicted as the positive class (true). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "Deleting model LightGBMXT_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\LightGBMXT_BAG_L1 will be removed.\n",
      "Deleting model LightGBM_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\LightGBM_BAG_L1 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\RandomForestGini_BAG_L1 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\RandomForestEntr_BAG_L1 will be removed.\n",
      "Deleting model CatBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\CatBoost_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\ExtraTreesGini_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\ExtraTreesEntr_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\NeuralNetFastAI_BAG_L1 will be removed.\n",
      "Deleting model XGBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\XGBoost_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\NeuralNetTorch_BAG_L1 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\LightGBMLarge_BAG_L1 will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\WeightedEnsemble_L2 will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\LightGBMXT_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\LightGBM_BAG_L2 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\RandomForestGini_BAG_L2 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\RandomForestEntr_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\CatBoost_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\ExtraTreesGini_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\ExtraTreesEntr_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\NeuralNetFastAI_BAG_L2 will be removed.\n",
      "Deleting model XGBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\XGBoost_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\NeuralNetTorch_BAG_L2 will be removed.\n",
      "Deleting model WeightedEnsemble_L3. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\\models\\WeightedEnsemble_L3 will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ie\\combined\")\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for combined:\n",
      "                           model  score_test score_val eval_metric  \\\n",
      "0   RandomForestEntr_BAG_L2_FULL    0.985571      None          f1   \n",
      "1   RandomForestGini_BAG_L2_FULL    0.985450      None          f1   \n",
      "2       WeightedEnsemble_L3_FULL    0.985384      None          f1   \n",
      "3    NeuralNetFastAI_BAG_L2_FULL    0.985351      None          f1   \n",
      "4           LightGBM_BAG_L1_FULL    0.982848      None          f1   \n",
      "5         LightGBMXT_BAG_L1_FULL    0.982693      None          f1   \n",
      "6            XGBoost_BAG_L1_FULL    0.975840      None          f1   \n",
      "7     ExtraTreesGini_BAG_L1_FULL    0.974455      None          f1   \n",
      "8   RandomForestGini_BAG_L1_FULL    0.973444      None          f1   \n",
      "9   RandomForestEntr_BAG_L1_FULL    0.973047      None          f1   \n",
      "10    ExtraTreesEntr_BAG_L1_FULL    0.972725      None          f1   \n",
      "11          CatBoost_BAG_L1_FULL    0.970850      None          f1   \n",
      "12    NeuralNetTorch_BAG_L1_FULL    0.964076      None          f1   \n",
      "13   NeuralNetFastAI_BAG_L1_FULL    0.957801      None          f1   \n",
      "14     LightGBMLarge_BAG_L1_FULL    0.951328      None          f1   \n",
      "\n",
      "    pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
      "0        22.579890            NaN  2709.706884                 0.672416   \n",
      "1        22.775125            NaN  2714.879564                 0.867651   \n",
      "2        28.097987            NaN  2936.730832                 0.023527   \n",
      "3        26.534393            NaN  2840.907745                 4.626919   \n",
      "4         3.803625            NaN    54.483497                 3.803625   \n",
      "5         3.561208            NaN    53.902269                 3.561208   \n",
      "6         2.238493            NaN    13.935526                 2.238493   \n",
      "7         1.143273      14.691549    19.468250                 1.143273   \n",
      "8         0.985517      14.067173    22.794389                 0.985517   \n",
      "9         0.933477      14.071002    22.290570                 0.933477   \n",
      "10        1.029918      14.547527    19.173720                 1.029918   \n",
      "11        0.317499            NaN  1804.999501                 0.317499   \n",
      "12        3.268758            NaN    52.041204                 3.268758   \n",
      "13        4.435868            NaN   622.005078                 4.435868   \n",
      "14        0.189839            NaN     2.519208                 0.189839   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                12.995988          22.093672            2       True   \n",
      "1                13.309402          27.266351            2       True   \n",
      "2                      NaN          46.463064            3       True   \n",
      "3                      NaN         153.294533            2       True   \n",
      "4                      NaN          54.483497            1       True   \n",
      "5                      NaN          53.902269            1       True   \n",
      "6                      NaN          13.935526            1       True   \n",
      "7                14.691549          19.468250            1       True   \n",
      "8                14.067173          22.794389            1       True   \n",
      "9                14.071002          22.290570            1       True   \n",
      "10               14.547527          19.173720            1       True   \n",
      "11                     NaN        1804.999501            1       True   \n",
      "12                     NaN          52.041204            1       True   \n",
      "13                     NaN         622.005078            1       True   \n",
      "14                     NaN           2.519208            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0          13  \n",
      "1          12  \n",
      "2          15  \n",
      "3          14  \n",
      "4           4  \n",
      "5           3  \n",
      "6           9  \n",
      "7           1  \n",
      "8           5  \n",
      "9           6  \n",
      "10          2  \n",
      "11          7  \n",
      "12         10  \n",
      "13          8  \n",
      "14         11  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:45:59.391915Z",
     "start_time": "2025-04-10T15:02:50.756427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_file = \"../data/ez/data_ez.csv\"\n",
    "ez_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.read_csv('../data/ez/data_ez_negative_sample.csv')\n",
    "}\n",
    "\n",
    "# For EI zone, our CSV files have columns B1 to B550.\n",
    "combined_data = prepare_data(ez_true_data, counter_files, max_index=550)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ez', 550, time_limit=21000)"
   ],
   "id": "afd161db397f19e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\AppData\\Local\\Temp\\ipykernel_15728\\4159827424.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ez_true_data = pd.read_csv(true_file)\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../models/ez/combined\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       5.08 GB / 15.35 GB (33.1%)\n",
      "Disk Space Avail:   45.24 GB / 100.00 GB (45.2%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality', 'optimize_for_deployment']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5250s of the 21000s of remaining time (25%).\n",
      "2025-04-10 11:02:55,900\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-04-10 11:03:03,846\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n",
      "\t\tContext path: \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Running DyStack sub-fit ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Beginning AutoGluon training ... Time limit = 5237s\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Train Data Rows:    26649\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Train Data Columns: 550\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Label Column:       label\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Problem Type:       binary\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Preprocessing data ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Using Feature Generators to preprocess the data ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tAvailable Memory:                    5854.81 MB\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTrain Data (Original)  Memory Usage: 810.72 MB (13.8% of available memory)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tWarning: Data size prior to feature transformation consumes 13.8% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tStage 1 Generators:\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tStage 2 Generators:\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tStage 3 Generators:\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tStage 4 Generators:\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tStage 5 Generators:\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\t('object', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t\t('category', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t19.4s = Fit runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t550 features in original data used to generate 550 features in processed data.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTrain Data (Processed) Memory Usage: 14.25 MB (0.2% of available memory)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Data preprocessing and feature engineering runtime = 19.9s ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m User-specified model hyperparameters to be fit:\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m {\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t'NN_TORCH': [{}],\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t'CAT': [{}],\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t'XGB': [{}],\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t'FASTAI': [{}],\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m }\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3477.14s of the 5217.02s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.82%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=30228)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.240281\tvalid_set's f1: 0.906331\n",
      "\u001B[36m(_ray_fit pid=13476)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.22908\tvalid_set's f1: 0.912018\u001B[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9114\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t116.78s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t4.86s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 3347.16s of the 5087.03s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.21%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=27592)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.225228\tvalid_set's f1: 0.913907\n",
      "\u001B[36m(_ray_fit pid=20084)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.243158\tvalid_set's f1: 0.909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9086\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t155.85s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t4.5s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 3183.49s of the 4923.37s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.8727\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t7.79s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.29s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3168.66s of the 4908.54s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.8768\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t7.4s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.26s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 3154.29s of the 4894.16s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.28%)\n",
      "\u001B[36m(_ray_fit pid=3688)\u001B[0m \tRan out of time, early stopping on iteration 366.\n",
      "\u001B[36m(_ray_fit pid=1336)\u001B[0m \tRan out of time, early stopping on iteration 382.\u001B[32m [repeated 5x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.8953\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t2516.16s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t4.38s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 630.54s of the 2370.42s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.8573\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.82s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.51s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 617.37s of the 2357.25s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=18896)\u001B[0m \tRan out of time, early stopping on iteration 388.\u001B[32m [repeated 2x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.8578\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.7s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.21s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 604.75s of the 2344.62s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.99%)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.8938\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t425.0s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t13.01s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 170.51s of the 1910.39s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.17%)\n",
      "\u001B[36m(_ray_fit pid=15636)\u001B[0m Warning: Large XGB model size may cause OOM error if training continues\n",
      "\u001B[36m(_ray_fit pid=15636)\u001B[0m Available Memory: 461 MB\n",
      "\u001B[36m(_ray_fit pid=15636)\u001B[0m Estimated XGB model size: 0 MB\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9111\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t136.36s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.61s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 26.11s of the 1765.98s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=3748)\u001B[0m Warning: Large XGB model size may cause OOM error if training continues\n",
      "\u001B[36m(_ray_fit pid=3748)\u001B[0m Available Memory: 330 MB\n",
      "\u001B[36m(_ray_fit pid=3748)\u001B[0m Estimated XGB model size: 0 MB\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.99%)\n",
      "\u001B[36m(_ray_fit pid=15800)\u001B[0m \tNot enough time to train first epoch. (Time Required: 36.49s, Time Left: 9.16s)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1738.45s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9114\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t3.14s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.02s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1735.23s of the 1735.19s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.28%)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9168\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t45.13s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t3.05s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1682.94s of the 1682.91s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=15392)\u001B[0m \tNot enough time to train first epoch. (Time Required: 35.82s, Time Left: 9.17s)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.41%)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9161\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t21.07s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t2.97s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1655.19s of the 1655.16s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001B[36mray::_ray_fit()\u001B[39m (pid=15800, ip=127.0.0.1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m     out = self._fit(**kwargs)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 227, in _fit\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m     self._train_net(\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 392, in _train_net\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m     raise TimeLimitExceeded\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9108\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.07s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.96s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1642.34s of the 1642.31s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9106\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.15s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.85s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1629.72s of the 1629.68s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.69%)\n",
      "\u001B[36m(_ray_fit pid=13872)\u001B[0m \tRan out of time, early stopping on iteration 205.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9165\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t1295.68s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t3.56s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 326.97s of the 326.94s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9106\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.17s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.07s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 314.95s of the 314.91s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=19788)\u001B[0m \tRan out of time, early stopping on iteration 212.\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9106\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t4.79s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.21s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 303.31s of the 303.28s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.95%)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.9077\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t189.65s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t13.55s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 91.16s of the 91.13s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.87%)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.919\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t72.61s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.06s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 11.05s of the 11.02s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.16%)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L2.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -11.83s of remaining time.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tEnsemble Weights: {'XGBoost_BAG_L2': 1.0}\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.919\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.23s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t0.0s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m AutoGluon training complete, total runtime = 5255.06s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 78.3 rows/s (3332 batch size)\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t17.07s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t14.51s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t7.79s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.29s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t7.4s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.26s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t609.73s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.82s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.51s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.7s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.21s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tStopping at the best epoch learned earlier - 9.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t204.08s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t14.22s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t3.14s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t4.65s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t2.81s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.07s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.96s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.15s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.85s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t286.11s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t5.17s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.07s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t4.79s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.21s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tStopping at the best epoch learned earlier - 1.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t43.24s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: XGBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t9.41s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tEnsemble Weights: {'XGBoost_BAG_L2': 1.0}\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \t6.23s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Updated best model to \"XGBoost_BAG_L2_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"XGBoost_BAG_L2_FULL\" for predict() and predict_proba().\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Refit complete, total runtime = 1214.4s ... Best model: \"XGBoost_BAG_L2_FULL\"\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tBase Threshold: 0.500\t| val: 0.9190\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m \tBest Threshold: 0.500\t| val: 0.9190\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=16424)\u001B[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0            XGBoost_BAG_L2_FULL       0.913544   0.918965          f1        5.697282            NaN   895.727085                 0.810598                     NaN           9.411391            2       True         19\n",
      "1       WeightedEnsemble_L3_FULL       0.913544   0.918965          f1        5.718845            NaN   901.956861                 0.021563                     NaN           6.229776            3       True         20\n",
      "2         LightGBMXT_BAG_L2_FULL       0.912878   0.916812          f1        5.331128            NaN   890.962520                 0.444443                     NaN           4.646827            2       True         11\n",
      "3           CatBoost_BAG_L2_FULL       0.912094   0.916542          f1        5.223680            NaN  1172.421391                 0.336996                     NaN         286.105697            2       True         15\n",
      "4           LightGBM_BAG_L2_FULL       0.909894   0.916055          f1        5.142256            NaN   889.129567                 0.255571                     NaN           2.813874            2       True         12\n",
      "5     ExtraTreesGini_BAG_L2_FULL       0.909195   0.910553          f1        5.335881            NaN   891.482610                 0.449197                6.068348           5.166917            2       True         16\n",
      "6   RandomForestGini_BAG_L2_FULL       0.907626   0.910768          f1        5.277284            NaN   892.389761                 0.390600                5.957706           6.074068            2       True         13\n",
      "7     ExtraTreesEntr_BAG_L2_FULL       0.907524   0.910625          f1        5.256205            NaN   891.103641                 0.369520                6.210369           4.787947            2       True         17\n",
      "8   RandomForestEntr_BAG_L2_FULL       0.907211   0.910585          f1        5.458491            NaN   892.462391                 0.571806                5.853829           6.146697            2       True         14\n",
      "9    NeuralNetFastAI_BAG_L2_FULL       0.906864   0.907726          f1        6.261342            NaN   929.553582                 1.374658                     NaN          43.237889            2       True         18\n",
      "10           XGBoost_BAG_L1_FULL       0.906743   0.911125          f1        0.639097            NaN    14.222650                 0.639097                     NaN          14.222650            1       True          9\n",
      "11          LightGBM_BAG_L1_FULL       0.905118   0.908621          f1        0.461502            NaN    14.510418                 0.461502                     NaN          14.510418            1       True          2\n",
      "12        LightGBMXT_BAG_L1_FULL       0.904638   0.911433          f1        0.261666            NaN    17.066711                 0.261666                     NaN          17.066711            1       True          1\n",
      "13      WeightedEnsemble_L2_FULL       0.904638   0.911433          f1        0.267709            NaN    20.210457                 0.006042                     NaN           3.143746            2       True         10\n",
      "14   NeuralNetFastAI_BAG_L1_FULL       0.889485   0.893828          f1        1.363173            NaN   204.075188                 1.363173                     NaN         204.075188            1       True          8\n",
      "15          CatBoost_BAG_L1_FULL       0.887283   0.895303          f1        0.382535            NaN   609.731022                 0.382535                     NaN         609.731022            1       True          5\n",
      "16  RandomForestGini_BAG_L1_FULL       0.883707   0.872701          f1        0.485968       6.285940     7.793761                 0.485968                6.285940           7.793761            1       True          3\n",
      "17  RandomForestEntr_BAG_L1_FULL       0.883172   0.876818          f1        0.413822       6.263723     7.401068                 0.413822                6.263723           7.401068            1       True          4\n",
      "18    ExtraTreesEntr_BAG_L1_FULL       0.870610   0.857752          f1        0.415624       6.212512     5.695346                 0.415624                6.212512           5.695346            1       True          7\n",
      "19    ExtraTreesGini_BAG_L1_FULL       0.863014   0.857265          f1        0.463297       6.506241     5.819530                 0.463297                6.506241           5.819530            1       True          6\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t6503s\t = DyStack   runtime |\t14497s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 14497s\n",
      "AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\"\n",
      "Train Data Rows:    29981\n",
      "Train Data Columns: 550\n",
      "Label Column:       label\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    7838.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 912.09 MB (11.6% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 11.6% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\t19.8s = Fit runtime\n",
      "\t550 features in original data used to generate 550 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 15.99 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 21.24s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 9647.83s of the 14475.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.90%)\n",
      "\t0.9136\t = Validation score   (f1)\n",
      "\t123.24s\t = Training   runtime\n",
      "\t5.32s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 9513.18s of the 14340.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.01%)\n",
      "\t0.9098\t = Validation score   (f1)\n",
      "\t80.26s\t = Training   runtime\n",
      "\t4.47s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 9426.11s of the 14253.65s of remaining time.\n",
      "\t0.8772\t = Validation score   (f1)\n",
      "\t8.07s\t = Training   runtime\n",
      "\t6.98s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 9410.31s of the 14237.85s of remaining time.\n",
      "\t0.8788\t = Validation score   (f1)\n",
      "\t7.64s\t = Training   runtime\n",
      "\t7.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 9394.72s of the 14222.25s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.59%)\n",
      "\t0.9028\t = Validation score   (f1)\n",
      "\t7508.94s\t = Training   runtime\n",
      "\t3.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1879.13s of the 6706.67s of remaining time.\n",
      "\t0.8649\t = Validation score   (f1)\n",
      "\t6.21s\t = Training   runtime\n",
      "\t7.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1865.08s of the 6692.61s of remaining time.\n",
      "\t0.8648\t = Validation score   (f1)\n",
      "\t6.27s\t = Training   runtime\n",
      "\t6.98s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1851.03s of the 6678.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.48%)\n",
      "\t0.8989\t = Validation score   (f1)\n",
      "\t900.48s\t = Training   runtime\n",
      "\t12.02s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 942.10s of the 5769.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.77%)\n",
      "\t0.9086\t = Validation score   (f1)\n",
      "\t123.3s\t = Training   runtime\n",
      "\t6.34s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 811.74s of the 5639.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.03%)\n",
      "\t0.8855\t = Validation score   (f1)\n",
      "\t650.01s\t = Training   runtime\n",
      "\t12.2s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 153.36s of the 4980.90s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.71%)\n",
      "\t0.9103\t = Validation score   (f1)\n",
      "\t84.53s\t = Training   runtime\n",
      "\t5.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 964.78s of the 4889.33s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.9136\t = Validation score   (f1)\n",
      "\t4.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4885.13s of the 4885.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.32%)\n",
      "\t0.9205\t = Validation score   (f1)\n",
      "\t29.41s\t = Training   runtime\n",
      "\t3.1s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4849.04s of the 4849.00s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.38%)\n",
      "\t0.9232\t = Validation score   (f1)\n",
      "\t18.97s\t = Training   runtime\n",
      "\t2.97s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 4823.50s of the 4823.46s of remaining time.\n",
      "\t0.9137\t = Validation score   (f1)\n",
      "\t6.71s\t = Training   runtime\n",
      "\t6.84s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 4809.25s of the 4809.21s of remaining time.\n",
      "\t0.9117\t = Validation score   (f1)\n",
      "\t6.55s\t = Training   runtime\n",
      "\t6.8s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4795.22s of the 4795.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.57%)\n",
      "\t0.9263\t = Validation score   (f1)\n",
      "\t3828.53s\t = Training   runtime\n",
      "\t3.43s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 959.91s of the 959.87s of remaining time.\n",
      "\t0.9118\t = Validation score   (f1)\n",
      "\t5.26s\t = Training   runtime\n",
      "\t6.85s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 947.09s of the 947.04s of remaining time.\n",
      "\t0.912\t = Validation score   (f1)\n",
      "\t5.37s\t = Training   runtime\n",
      "\t6.83s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 934.20s of the 934.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.15%)\n",
      "\t0.9149\t = Validation score   (f1)\n",
      "\t628.2s\t = Training   runtime\n",
      "\t12.07s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 298.61s of the 298.56s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.09%)\n",
      "\t0.9221\t = Validation score   (f1)\n",
      "\t101.99s\t = Training   runtime\n",
      "\t6.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 189.56s of the 189.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.15%)\n",
      "\t0.9088\t = Validation score   (f1)\n",
      "\t140.26s\t = Training   runtime\n",
      "\t11.42s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 40.78s of the 40.74s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=6.45%)\n",
      "\t0.9228\t = Validation score   (f1)\n",
      "\t32.86s\t = Training   runtime\n",
      "\t3.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 488.51s of the 0.66s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.55, 'RandomForestGini_BAG_L1': 0.1, 'RandomForestEntr_BAG_L1': 0.1, 'ExtraTreesEntr_BAG_L1': 0.1, 'LightGBMLarge_BAG_L2': 0.1, 'ExtraTreesGini_BAG_L1': 0.05}\n",
      "\t0.9267\t = Validation score   (f1)\n",
      "\t8.28s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 14504.34s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 64.1 rows/s (3748 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t8.07s\t = Training   runtime\n",
      "\t6.98s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t7.64s\t = Training   runtime\n",
      "\t7.16s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t6.21s\t = Training   runtime\n",
      "\t7.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t6.27s\t = Training   runtime\n",
      "\t6.98s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t20.68s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t15.19s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t1464.56s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t440.13s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t13.99s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t217.07s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t21.84s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\t951.56s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L2_FULL ...\n",
      "\t5.03s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.55, 'RandomForestGini_BAG_L1': 0.1, 'RandomForestEntr_BAG_L1': 0.1, 'ExtraTreesEntr_BAG_L1': 0.1, 'LightGBMLarge_BAG_L2': 0.1, 'ExtraTreesGini_BAG_L1': 0.05}\n",
      "\t8.28s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 3156.77s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.9267\n",
      "\tBest Threshold: 0.501\t| val: 0.9267\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.501\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.501 will be predicted as the positive class (true). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "Deleting model LightGBMXT_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\LightGBMXT_BAG_L1 will be removed.\n",
      "Deleting model LightGBM_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\LightGBM_BAG_L1 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\RandomForestGini_BAG_L1 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\RandomForestEntr_BAG_L1 will be removed.\n",
      "Deleting model CatBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\CatBoost_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\ExtraTreesGini_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\ExtraTreesEntr_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\NeuralNetFastAI_BAG_L1 will be removed.\n",
      "Deleting model XGBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\XGBoost_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\NeuralNetTorch_BAG_L1 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\LightGBMLarge_BAG_L1 will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\WeightedEnsemble_L2 will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\LightGBMXT_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\LightGBM_BAG_L2 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\RandomForestGini_BAG_L2 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\RandomForestEntr_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\CatBoost_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\ExtraTreesGini_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\ExtraTreesEntr_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\NeuralNetFastAI_BAG_L2 will be removed.\n",
      "Deleting model XGBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\XGBoost_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\NeuralNetTorch_BAG_L2 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\LightGBMLarge_BAG_L2 will be removed.\n",
      "Deleting model WeightedEnsemble_L3. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\\models\\WeightedEnsemble_L3 will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ez\\combined\")\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for combined:\n",
      "                           model  score_test score_val eval_metric  \\\n",
      "0       WeightedEnsemble_L3_FULL    0.929105      None          f1   \n",
      "1           CatBoost_BAG_L2_FULL    0.928802      None          f1   \n",
      "2      LightGBMLarge_BAG_L2_FULL    0.922367      None          f1   \n",
      "3         LightGBMXT_BAG_L1_FULL    0.913837      None          f1   \n",
      "4            XGBoost_BAG_L1_FULL    0.912855      None          f1   \n",
      "5           LightGBM_BAG_L1_FULL    0.910082      None          f1   \n",
      "6      LightGBMLarge_BAG_L1_FULL    0.909850      None          f1   \n",
      "7           CatBoost_BAG_L1_FULL    0.909310      None          f1   \n",
      "8   RandomForestEntr_BAG_L1_FULL    0.882910      None          f1   \n",
      "9   RandomForestGini_BAG_L1_FULL    0.879931      None          f1   \n",
      "10    ExtraTreesEntr_BAG_L1_FULL    0.875536      None          f1   \n",
      "11   NeuralNetFastAI_BAG_L1_FULL    0.875286      None          f1   \n",
      "12    ExtraTreesGini_BAG_L1_FULL    0.873179      None          f1   \n",
      "13    NeuralNetTorch_BAG_L1_FULL    0.871875      None          f1   \n",
      "\n",
      "    pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
      "0        13.554013            NaN  3186.525345                 0.021594   \n",
      "1        13.223663            NaN  3173.217144                 0.418056   \n",
      "2        13.114362            NaN  2226.682844                 0.308755   \n",
      "3         0.470728            NaN    20.677453                 0.470728   \n",
      "4         1.570292            NaN    13.991075                 1.570292   \n",
      "5         0.419377            NaN    15.189304                 0.419377   \n",
      "6         0.503284            NaN    21.841905                 0.503284   \n",
      "7         0.406080            NaN  1464.564611                 0.406080   \n",
      "8         0.611511       7.160988     7.635108                 0.611511   \n",
      "9         0.653521       6.979568     8.071974                 0.653521   \n",
      "10        0.524062       6.982081     6.269431                 0.524062   \n",
      "11        3.512261            NaN   440.131761                 3.512261   \n",
      "12        0.828244       7.041595     6.210447                 0.828244   \n",
      "13        3.306247            NaN   217.074150                 3.306247   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                      NaN           8.282577            3       True   \n",
      "1                      NaN         951.559924            2       True   \n",
      "2                      NaN           5.025625            2       True   \n",
      "3                      NaN          20.677453            1       True   \n",
      "4                      NaN          13.991075            1       True   \n",
      "5                      NaN          15.189304            1       True   \n",
      "6                      NaN          21.841905            1       True   \n",
      "7                      NaN        1464.564611            1       True   \n",
      "8                 7.160988           7.635108            1       True   \n",
      "9                 6.979568           8.071974            1       True   \n",
      "10                6.982081           6.269431            1       True   \n",
      "11                     NaN         440.131761            1       True   \n",
      "12                7.041595           6.210447            1       True   \n",
      "13                     NaN         217.074150            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0          14  \n",
      "1          12  \n",
      "2          13  \n",
      "3           5  \n",
      "4           9  \n",
      "5           6  \n",
      "6          11  \n",
      "7           7  \n",
      "8           2  \n",
      "9           1  \n",
      "10          4  \n",
      "11          8  \n",
      "12          3  \n",
      "13         10  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T21:48:39.077495Z",
     "start_time": "2025-04-11T15:11:18.023431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_file = \"../data/ze/data_ze.csv\"\n",
    "ze_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.read_csv('../data/ze/data_ze_negative_sample.csv')\n",
    "}\n",
    "\n",
    "# For EZ zone, our CSV files have columns B1 to B550.\n",
    "combined_data = prepare_data(ze_true_data, counter_files, max_index=550)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ze', 550, time_limit=21000)"
   ],
   "id": "c9bf13ad02b3d5c0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriel\\AppData\\Local\\Temp\\ipykernel_29112\\3893956261.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ze_true_data = pd.read_csv(true_file)\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../models/ze/combined\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       4.46 GB / 15.35 GB (29.0%)\n",
      "Disk Space Avail:   44.66 GB / 100.00 GB (44.7%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality', 'optimize_for_deployment']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5250s of the 21000s of remaining time (25%).\n",
      "2025-04-11 11:11:22,414\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-04-11 11:11:30,584\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n",
      "\t\tContext path: \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Running DyStack sub-fit ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Beginning AutoGluon training ... Time limit = 5238s\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Train Data Rows:    26649\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Train Data Columns: 550\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Label Column:       label\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Problem Type:       binary\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Preprocessing data ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Using Feature Generators to preprocess the data ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tAvailable Memory:                    4449.10 MB\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTrain Data (Original)  Memory Usage: 810.72 MB (18.2% of available memory)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tWarning: Data size prior to feature transformation consumes 18.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tStage 1 Generators:\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tStage 2 Generators:\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tStage 3 Generators:\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tStage 4 Generators:\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tStage 5 Generators:\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\t('object', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t\t('category', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t17.7s = Fit runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t550 features in original data used to generate 550 features in processed data.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTrain Data (Processed) Memory Usage: 14.25 MB (0.3% of available memory)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Data preprocessing and feature engineering runtime = 18.27s ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m User-specified model hyperparameters to be fit:\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m {\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t'NN_TORCH': [{}],\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t'CAT': [{}],\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t'XGB': [{}],\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t'FASTAI': [{}],\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m }\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3478.79s of the 5219.50s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.53%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=14628)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.244408\tvalid_set's f1: 0.911004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9076\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t87.82s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t4.35s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 3379.84s of the 5120.55s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.86%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=27960)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.237465\tvalid_set's f1: 0.9121\u001B[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.908\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t104.13s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t4.4s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 3267.84s of the 5008.55s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.8762\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.27s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3253.88s of the 4994.58s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.8799\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t7.08s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.17s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 3239.91s of the 4980.62s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.85%)\n",
      "\u001B[36m(_ray_fit pid=2800)\u001B[0m Warning: Large model size may cause OOM error if training continues\n",
      "\u001B[36m(_ray_fit pid=2800)\u001B[0m Warning: Low available memory may cause OOM error if training continues\n",
      "\u001B[36m(_ray_fit pid=2800)\u001B[0m Warning: Early stopped model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\n",
      "\u001B[36m(_ray_fit pid=2800)\u001B[0m Available Memory: 479 MB, Estimated Model size: 582 MB\n",
      "\u001B[36m(_ray_fit pid=2800)\u001B[0m \tRan low on memory, early stopping on iteration 30.\n",
      "\u001B[36m(_ray_fit pid=19780)\u001B[0m \tRan out of time, early stopping on iteration 431.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.8906\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t2584.57s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t3.52s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 648.40s of the 2389.10s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.8659\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.8s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.34s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 635.49s of the 2376.19s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=23132)\u001B[0m \tRan out of time, early stopping on iteration 433.\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.8648\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.58s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.18s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 623.02s of the 2363.73s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.01%)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.8935\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t395.25s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t11.96s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 206.85s of the 1947.55s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.72%)\n",
      "\u001B[36m(_ray_fit pid=8772)\u001B[0m Warning: Large XGB model size may cause OOM error if training continues\n",
      "\u001B[36m(_ray_fit pid=8772)\u001B[0m Available Memory: 20 MB\n",
      "\u001B[36m(_ray_fit pid=8772)\u001B[0m Estimated XGB model size: 4 MB\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9042\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t162.41s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.84s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 37.41s of the 1778.11s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.10%)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "\u001B[36m(_ray_fit pid=23480)\u001B[0m \tNot enough time to train first epoch. (Time Required: 34.94s, Time Left: 19.41s)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 12.08s of the 1752.78s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=8.29%)\n",
      "\u001B[36m(_ray_fit pid=5452)\u001B[0m \tRan out of time, early stopping on iteration 74. Best iteration is:\n",
      "\u001B[36m(_ray_fit pid=5452)\u001B[0m \t[74]\tvalid_set's binary_logloss: 0.332865\tvalid_set's f1: 0.888511\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.8896\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t10.12s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t3.83s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1735.76s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.908\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t3.8s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.02s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1731.89s of the 1731.86s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.21%)\n",
      "\u001B[36m(_ray_fit pid=10260)\u001B[0m \tRan out of time, early stopping on iteration 69. Best iteration is:\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=10260)\u001B[0m \t[69]\tvalid_set's binary_logloss: 0.330625\tvalid_set's f1: 0.892501\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.912\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t32.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t2.65s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1692.72s of the 1692.69s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.29%)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9108\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t22.56s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t2.69s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1663.63s of the 1663.60s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9098\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.88s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.07s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1650.88s of the 1650.85s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9102\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.99s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.97s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1638.29s of the 1638.26s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.05%)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_ray_fit pid=11860)\u001B[0m \tRan out of time, early stopping on iteration 229.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9103\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t1303.83s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t3.54s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 327.70s of the 327.67s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=19724)\u001B[0m \tRan out of time, early stopping on iteration 225.\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9093\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t4.82s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.13s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 316.11s of the 316.08s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.909\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t4.66s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.01s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 304.68s of the 304.64s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.93%)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.9038\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t210.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t12.21s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 85.30s of the 85.27s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.09%)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.912\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t65.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.99s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 12.67s of the 12.64s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.03%)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L2.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -9.20s of remaining time.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 1.0}\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.912\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.42s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t0.0s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m AutoGluon training complete, total runtime = 5253.51s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 84.0 rows/s (3332 batch size)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001B[36mray::_ray_fit()\u001B[39m (pid=2140, ip=127.0.0.1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m     out = self._fit(**kwargs)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 224, in _fit\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m     raise TimeLimitExceeded\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001B[36mray::_ray_fit()\u001B[39m (pid=16140, ip=127.0.0.1)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"python\\ray\\_raylet.pyx\", line 1862, in ray._raylet.execute_task\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 925, in fit\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m     out = self._fit(**kwargs)\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   File \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\autogluon\\tabular\\models\\tabular_nn\\torch\\tabular_nn_torch.py\", line 224, in _fit\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m     raise TimeLimitExceeded\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m autogluon.core.utils.exceptions.TimeLimitExceeded\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t13.63s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t14.42s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.27s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t7.08s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.17s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t602.39s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.8s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.34s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.58s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.18s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tStopping at the best epoch learned earlier - 9.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t188.92s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t12.87s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t3.02s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t3.8s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t3.06s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t1.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.88s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.07s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.99s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t5.97s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t294.32s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t4.82s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.13s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t4.66s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.01s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tStopping at the best epoch learned earlier - 1.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t40.65s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: XGBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t7.78s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 1.0}\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \t6.42s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Updated best model to \"LightGBMXT_BAG_L2_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"LightGBMXT_BAG_L2_FULL\" for predict() and predict_proba().\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Refit complete, total runtime = 1191.87s ... Best model: \"LightGBMXT_BAG_L2_FULL\"\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tBase Threshold: 0.500\t| val: 0.9120\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m \tBest Threshold: 0.500\t| val: 0.9120\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=16408)\u001B[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0   RandomForestGini_BAG_L2_FULL       0.902880   0.909781          f1        5.583936            NaN   866.544179                 0.391882                6.069077           5.877188            2       True         14\n",
      "1           LightGBM_BAG_L2_FULL       0.902315   0.910831          f1        5.601332            NaN   862.629848                 0.409277                     NaN           1.962856            2       True         13\n",
      "2     ExtraTreesEntr_BAG_L2_FULL       0.901728   0.908988          f1        5.560374            NaN   865.325365                 0.368319                6.012830           4.658373            2       True         18\n",
      "3     ExtraTreesGini_BAG_L2_FULL       0.901672   0.909343          f1        5.735979            NaN   865.486748                 0.543924                6.129231           4.819757            2       True         17\n",
      "4   RandomForestEntr_BAG_L2_FULL       0.901440   0.910159          f1        5.559172            NaN   866.659456                 0.367118                5.969438           5.992465            2       True         15\n",
      "5            XGBoost_BAG_L2_FULL       0.901440   0.911998          f1        5.836474            NaN   868.442022                 0.644419                     NaN           7.775030            2       True         20\n",
      "6    NeuralNetFastAI_BAG_L2_FULL       0.901401   0.903802          f1        6.517403            NaN   901.320417                 1.325349                     NaN          40.653425            2       True         19\n",
      "7            XGBoost_BAG_L1_FULL       0.901392   0.904233          f1        0.608812            NaN    12.871832                 0.608812                     NaN          12.871832            1       True          9\n",
      "8         LightGBMXT_BAG_L1_FULL       0.901049   0.907598          f1        0.270389            NaN    13.631290                 0.270389                     NaN          13.631290            1       True          1\n",
      "9         LightGBMXT_BAG_L2_FULL       0.900681   0.911999          f1        5.446641            NaN   863.724968                 0.254586                     NaN           3.057976            2       True         12\n",
      "10      WeightedEnsemble_L3_FULL       0.900681   0.911999          f1        5.466489            NaN   870.148523                 0.019848                     NaN           6.423555            3       True         21\n",
      "11          LightGBM_BAG_L1_FULL       0.900114   0.908036          f1        0.460700            NaN    14.420172                 0.460700                     NaN          14.420172            1       True          2\n",
      "12      WeightedEnsemble_L2_FULL       0.900114   0.908036          f1        0.464720            NaN    18.216544                 0.004020                     NaN           3.796372            2       True         11\n",
      "13          CatBoost_BAG_L2_FULL       0.899660   0.910336          f1        5.516303            NaN  1154.985341                 0.324248                     NaN         294.318349            2       True         16\n",
      "14   NeuralNetFastAI_BAG_L1_FULL       0.887903   0.893469          f1        1.310573            NaN   188.915961                 1.310573                     NaN         188.915961            1       True          8\n",
      "15          CatBoost_BAG_L1_FULL       0.886409   0.890574          f1        0.328362            NaN   602.389411                 0.328362                     NaN         602.389411            1       True          5\n",
      "16  RandomForestEntr_BAG_L1_FULL       0.885866   0.879866          f1        0.462950       6.168024     7.082595                 0.462950                6.168024           7.082595            1       True          4\n",
      "17  RandomForestGini_BAG_L1_FULL       0.882528   0.876157          f1        0.510015       6.274117     6.962388                 0.510015                6.274117           6.962388            1       True          3\n",
      "18     LightGBMLarge_BAG_L1_FULL       0.881298   0.889619          f1        0.400255            NaN     3.016081                 0.400255                     NaN           3.016081            1       True         10\n",
      "19    ExtraTreesEntr_BAG_L1_FULL       0.872585   0.864808          f1        0.416141       6.177729     5.582051                 0.416141                6.177729           5.582051            1       True          7\n",
      "20    ExtraTreesGini_BAG_L1_FULL       0.869565   0.865945          f1        0.423858       6.341624     5.795212                 0.423858                6.341624           5.795212            1       True          6\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t6477s\t = DyStack   runtime |\t14523s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 14523s\n",
      "AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\"\n",
      "Train Data Rows:    29981\n",
      "Train Data Columns: 550\n",
      "Label Column:       label\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    8973.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 912.09 MB (10.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 10.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 550 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\t17.8s = Fit runtime\n",
      "\t550 features in original data used to generate 550 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 15.99 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 18.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 9667.19s of the 14504.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.56%)\n",
      "\t0.9102\t = Validation score   (f1)\n",
      "\t136.21s\t = Training   runtime\n",
      "\t5.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 9519.48s of the 14356.70s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.59%)\n",
      "\t0.9089\t = Validation score   (f1)\n",
      "\t123.37s\t = Training   runtime\n",
      "\t5.14s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 9389.09s of the 14226.31s of remaining time.\n",
      "\t0.8803\t = Validation score   (f1)\n",
      "\t8.24s\t = Training   runtime\n",
      "\t6.95s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 9373.16s of the 14210.38s of remaining time.\n",
      "\t0.8844\t = Validation score   (f1)\n",
      "\t7.9s\t = Training   runtime\n",
      "\t6.96s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 9357.58s of the 14194.80s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.12%)\n",
      "\t0.8994\t = Validation score   (f1)\n",
      "\t7477.53s\t = Training   runtime\n",
      "\t3.64s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1873.00s of the 6710.23s of remaining time.\n",
      "\t0.8689\t = Validation score   (f1)\n",
      "\t6.24s\t = Training   runtime\n",
      "\t7.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1858.80s of the 6696.02s of remaining time.\n",
      "\t0.8683\t = Validation score   (f1)\n",
      "\t6.52s\t = Training   runtime\n",
      "\t7.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1844.50s of the 6681.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.15%)\n",
      "\t0.9008\t = Validation score   (f1)\n",
      "\t904.62s\t = Training   runtime\n",
      "\t12.32s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 919.36s of the 5756.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.39%)\n",
      "\t0.9047\t = Validation score   (f1)\n",
      "\t133.62s\t = Training   runtime\n",
      "\t6.47s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 778.62s of the 5615.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.09%)\n",
      "\t0.8849\t = Validation score   (f1)\n",
      "\t615.09s\t = Training   runtime\n",
      "\t11.81s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 154.85s of the 4992.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=5.87%)\n",
      "\t0.9087\t = Validation score   (f1)\n",
      "\t100.54s\t = Training   runtime\n",
      "\t5.15s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 966.72s of the 4884.31s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.955, 'ExtraTreesGini_BAG_L1': 0.045}\n",
      "\t0.9103\t = Validation score   (f1)\n",
      "\t4.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 4880.15s of the 4880.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.36%)\n",
      "\t0.9126\t = Validation score   (f1)\n",
      "\t26.99s\t = Training   runtime\n",
      "\t2.63s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4846.46s of the 4846.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.39%)\n",
      "\t0.9112\t = Validation score   (f1)\n",
      "\t26.7s\t = Training   runtime\n",
      "\t2.66s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 4813.08s of the 4813.03s of remaining time.\n",
      "\t0.9102\t = Validation score   (f1)\n",
      "\t6.6s\t = Training   runtime\n",
      "\t6.78s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 4798.83s of the 4798.79s of remaining time.\n",
      "\t0.9107\t = Validation score   (f1)\n",
      "\t6.84s\t = Training   runtime\n",
      "\t6.89s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4784.43s of the 4784.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=3.64%)\n",
      "\t0.9117\t = Validation score   (f1)\n",
      "\t3818.99s\t = Training   runtime\n",
      "\t3.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 958.65s of the 958.61s of remaining time.\n",
      "\t0.91\t = Validation score   (f1)\n",
      "\t5.29s\t = Training   runtime\n",
      "\t7.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 945.60s of the 945.56s of remaining time.\n",
      "\t0.91\t = Validation score   (f1)\n",
      "\t5.29s\t = Training   runtime\n",
      "\t6.85s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 932.76s of the 932.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=2.29%)\n",
      "\t0.9096\t = Validation score   (f1)\n",
      "\t617.73s\t = Training   runtime\n",
      "\t12.09s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 307.20s of the 307.16s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=4.54%)\n",
      "\t0.9105\t = Validation score   (f1)\n",
      "\t82.05s\t = Training   runtime\n",
      "\t6.28s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 217.89s of the 217.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.21%)\n",
      "\t0.9055\t = Validation score   (f1)\n",
      "\t180.03s\t = Training   runtime\n",
      "\t11.89s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 28.93s of the 28.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=8.68%)\n",
      "\t0.9115\t = Validation score   (f1)\n",
      "\t23.31s\t = Training   runtime\n",
      "\t2.71s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 488.02s of the -1.83s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 1.0}\n",
      "\t0.9126\t = Validation score   (f1)\n",
      "\t8.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 14533.18s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 67.2 rows/s (3748 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t20.15s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t19.32s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t8.24s\t = Training   runtime\n",
      "\t6.95s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t7.9s\t = Training   runtime\n",
      "\t6.96s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t1981.16s\t = Training   runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t6.24s\t = Training   runtime\n",
      "\t7.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t6.52s\t = Training   runtime\n",
      "\t7.03s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\tStopping at the best epoch learned earlier - 19.\n",
      "\t486.87s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t17.37s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t245.36s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t23.1s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\t3.71s\t = Training   runtime\n",
      "Updated best model to \"LightGBMXT_BAG_L2_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"LightGBMXT_BAG_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 2804.46s ... Best model: \"LightGBMXT_BAG_L2_FULL\"\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.9126\n",
      "\tBest Threshold: 0.499\t| val: 0.9126\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.499\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.499 will be predicted as the positive class (true). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "Deleting model LightGBMXT_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\LightGBMXT_BAG_L1 will be removed.\n",
      "Deleting model LightGBM_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\LightGBM_BAG_L1 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\RandomForestGini_BAG_L1 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\RandomForestEntr_BAG_L1 will be removed.\n",
      "Deleting model CatBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\CatBoost_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\ExtraTreesGini_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\ExtraTreesEntr_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\NeuralNetFastAI_BAG_L1 will be removed.\n",
      "Deleting model XGBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\XGBoost_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\NeuralNetTorch_BAG_L1 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\LightGBMLarge_BAG_L1 will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\WeightedEnsemble_L2 will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\LightGBMXT_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\LightGBM_BAG_L2 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\RandomForestGini_BAG_L2 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\RandomForestEntr_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\CatBoost_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\ExtraTreesGini_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\ExtraTreesEntr_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\NeuralNetFastAI_BAG_L2 will be removed.\n",
      "Deleting model XGBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\XGBoost_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\NeuralNetTorch_BAG_L2 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\LightGBMLarge_BAG_L2 will be removed.\n",
      "Deleting model WeightedEnsemble_L3. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\\models\\WeightedEnsemble_L3 will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ze\\combined\")\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for combined:\n",
      "                           model  score_test score_val eval_metric  \\\n",
      "0         LightGBMXT_BAG_L1_FULL    0.911691      None          f1   \n",
      "1         LightGBMXT_BAG_L2_FULL    0.911343      None          f1   \n",
      "2      LightGBMLarge_BAG_L1_FULL    0.911308      None          f1   \n",
      "3            XGBoost_BAG_L1_FULL    0.910693      None          f1   \n",
      "4           LightGBM_BAG_L1_FULL    0.909899      None          f1   \n",
      "5           CatBoost_BAG_L1_FULL    0.901141      None          f1   \n",
      "6    NeuralNetFastAI_BAG_L1_FULL    0.896934      None          f1   \n",
      "7   RandomForestEntr_BAG_L1_FULL    0.884800      None          f1   \n",
      "8   RandomForestGini_BAG_L1_FULL    0.884798      None          f1   \n",
      "9     NeuralNetTorch_BAG_L1_FULL    0.880514      None          f1   \n",
      "10    ExtraTreesEntr_BAG_L1_FULL    0.876652      None          f1   \n",
      "11    ExtraTreesGini_BAG_L1_FULL    0.874442      None          f1   \n",
      "\n",
      "    pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
      "0         0.812123            NaN    20.146281                 0.812123   \n",
      "1        15.668938            NaN  2825.940213                 0.352938   \n",
      "2         0.563599            NaN    23.096215                 0.563599   \n",
      "3         2.073780            NaN    17.374912                 2.073780   \n",
      "4         0.897300            NaN    19.324649                 0.897300   \n",
      "5         0.504236            NaN  1981.157988                 0.504236   \n",
      "6         4.167193            NaN   486.867028                 4.167193   \n",
      "7         0.656197       6.955056     7.897766                 0.656197   \n",
      "8         0.699276       6.949152     8.235774                 0.699276   \n",
      "9         3.588232            NaN   245.364949                 3.588232   \n",
      "10        0.718378       7.030248     6.520911                 0.718378   \n",
      "11        0.635688       7.045423     6.238741                 0.635688   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                      NaN          20.146281            1       True   \n",
      "1                      NaN           3.714999            2       True   \n",
      "2                      NaN          23.096215            1       True   \n",
      "3                      NaN          17.374912            1       True   \n",
      "4                      NaN          19.324649            1       True   \n",
      "5                      NaN        1981.157988            1       True   \n",
      "6                      NaN         486.867028            1       True   \n",
      "7                 6.955056           7.897766            1       True   \n",
      "8                 6.949152           8.235774            1       True   \n",
      "9                      NaN         245.364949            1       True   \n",
      "10                7.030248           6.520911            1       True   \n",
      "11                7.045423           6.238741            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0           1  \n",
      "1          12  \n",
      "2          11  \n",
      "3           9  \n",
      "4           2  \n",
      "5           5  \n",
      "6           8  \n",
      "7           4  \n",
      "8           3  \n",
      "9          10  \n",
      "10          7  \n",
      "11          6  \n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
