{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-28T15:30:07.803787Z",
     "start_time": "2025-03-28T15:30:07.791589Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:30:09.438545Z",
     "start_time": "2025-03-28T15:30:09.376764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove models folder if it exists\n",
    "import shutil\n",
    "shutil.rmtree(\"../models\", ignore_errors=True)"
   ],
   "id": "6c0e6a43ad138192",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:30:11.494817Z",
     "start_time": "2025-03-28T15:30:11.479030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(true_data, counter_files, max_index):\n",
    "    combined_data = {}\n",
    "    # Define the column names to keep.\n",
    "    cols = [f\"B{i}\" for i in range(1, max_index + 1)]\n",
    "\n",
    "    # Process the true data: retain only the B columns and add the true label.\n",
    "    true_data['label'] = \"true\"\n",
    "    true_data = true_data[cols + [\"label\"]]\n",
    "\n",
    "    for key, df_counter in counter_files.items():\n",
    "        # Retain only the B columns and add the false label.\n",
    "        df_counter['label'] = \"false\"\n",
    "        df_counter = df_counter[cols + [\"label\"]]\n",
    "        # Combine the true data with this counter example.\n",
    "        combined = pd.concat([true_data, df_counter], ignore_index=True)\n",
    "        combined_data[key] = combined\n",
    "\n",
    "    return combined_data"
   ],
   "id": "1ba82cefa1f193c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:30:13.537126Z",
     "start_time": "2025-03-28T15:30:13.525238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "presets = ['good_quality', 'optimize_for_deployment']\n",
    "\n",
    "def generateModels(combined_data, prefix, max_index, time_limit=180):\n",
    "    results = {}\n",
    "    # Define the feature columns\n",
    "    feature_cols = [f\"B{i}\" for i in range(1, max_index + 1)]\n",
    "\n",
    "    for key, df in combined_data.items():\n",
    "        # Split data into features and label.\n",
    "        X = df[feature_cols]\n",
    "        y = df[\"label\"]\n",
    "\n",
    "        # Train-test split: 70% train, 30% test.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "        train_data = pd.concat([X_train, y_train], axis=1)\n",
    "        test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        # Define an output folder for saving models for this dataset variant.\n",
    "        output_folder = f\"../models/{prefix}/{key}\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Train the model using AutoGluon.\n",
    "        predictor = TabularPredictor(label=\"label\", path=output_folder, eval_metric='f1').fit(train_data, presets=presets, time_limit=time_limit)\n",
    "\n",
    "        # Evaluate the model on the test data.\n",
    "        leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "        results[key] = leaderboard\n",
    "\n",
    "        print(f\"Results for {key}:\")\n",
    "        print(leaderboard)\n",
    "\n",
    "    # Create results folder and save leaderboards.\n",
    "    results_folder = f\"../models/{prefix}/results\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "    for key, result in results.items():\n",
    "        result.to_csv(os.path.join(results_folder, f\"{key}_leaderboard.csv\"), index=False)"
   ],
   "id": "81f67c0ce624347c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:14:20.291078Z",
     "start_time": "2025-03-28T15:30:42.156225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "true_file = \"../data/ei/data_ei.csv\"\n",
    "ei_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\"../data/ei/data_ei_random.csv\"),\n",
    "            pd.read_csv(\"../data/ei/data_ez_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ei/data_ie_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ei/data_ze_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ei/data_ie_true_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ei/data_test_false.csv\"),\n",
    "         ]\n",
    "    )\n",
    "}\n",
    "\n",
    "# For EI zone, our CSV files have columns B1 to B12.\n",
    "combined_data = prepare_data(ei_true_data, counter_files, max_index=12)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ei', 12, time_limit=21600)"
   ],
   "id": "2d84d45e4f7d2f32",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../models/ei/combined\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          16\n",
      "Memory Avail:       6.45 GB / 15.35 GB (42.0%)\n",
      "Disk Space Avail:   47.75 GB / 100.00 GB (47.7%)\n",
      "===================================================\n",
      "Presets specified: ['good_quality', 'optimize_for_deployment']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5400s of the 21600s of remaining time (25%).\n",
      "2025-03-28 11:30:43,338\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-03-28 11:30:48,446\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n",
      "\t\tContext path: \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Running DyStack sub-fit ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Beginning AutoGluon training ... Time limit = 5390s\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Train Data Rows:    85576\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Train Data Columns: 12\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Label Column:       label\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Problem Type:       binary\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Preprocessing data ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Using Feature Generators to preprocess the data ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tAvailable Memory:                    5990.04 MB\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTrain Data (Original)  Memory Usage: 56.80 MB (0.9% of available memory)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 1 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 2 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 3 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 4 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tStage 5 Generators:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\t('object', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t\t('category', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.4s = Fit runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t12 features in original data used to generate 12 features in processed data.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTrain Data (Processed) Memory Usage: 0.99 MB (0.0% of available memory)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Data preprocessing and feature engineering runtime = 0.4s ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m User-specified model hyperparameters to be fit:\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m {\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'NN_TORCH': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'CAT': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'XGB': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'FASTAI': [{}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m }\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3592.32s of the 5389.83s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.063705\tvalid_set's f1: 0.957524\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [2000]\tvalid_set's binary_logloss: 0.0596066\tvalid_set's f1: 0.962963\u001B[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [3000]\tvalid_set's binary_logloss: 0.058624\tvalid_set's f1: 0.963412\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [4000]\tvalid_set's binary_logloss: 0.0589464\tvalid_set's f1: 0.965462\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [5000]\tvalid_set's binary_logloss: 0.0598116\tvalid_set's f1: 0.965489\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [6000]\tvalid_set's binary_logloss: 0.0608523\tvalid_set's f1: 0.966505\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [7000]\tvalid_set's binary_logloss: 0.0619875\tvalid_set's f1: 0.966714\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [8000]\tvalid_set's binary_logloss: 0.0635435\tvalid_set's f1: 0.966895\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [9000]\tvalid_set's binary_logloss: 0.06511\tvalid_set's f1: 0.967286\u001B[32m [repeated 4x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=22400)\u001B[0m [10000]\tvalid_set's binary_logloss: 0.0663259\tvalid_set's f1: 0.966882\u001B[32m [repeated 3x across cluster]\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9675\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t132.8s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t51.79s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 3442.75s of the 5240.25s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.0611877\tvalid_set's f1: 0.960582\n",
      "\u001B[36m(_ray_fit pid=24548)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.0657267\tvalid_set's f1: 0.955569\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [2000]\tvalid_set's binary_logloss: 0.0575243\tvalid_set's f1: 0.965253\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [3000]\tvalid_set's binary_logloss: 0.0570503\tvalid_set's f1: 0.96631\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [4000]\tvalid_set's binary_logloss: 0.0580491\tvalid_set's f1: 0.96713\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=8328)\u001B[0m [5000]\tvalid_set's binary_logloss: 0.059545\tvalid_set's f1: 0.966895\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=20476)\u001B[0m [6000]\tvalid_set's binary_logloss: 0.0584109\tvalid_set's f1: 0.966465\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [7000]\tvalid_set's binary_logloss: 0.061186\tvalid_set's f1: 0.969855\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [8000]\tvalid_set's binary_logloss: 0.0626911\tvalid_set's f1: 0.970636\u001B[32m [repeated 4x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [9000]\tvalid_set's binary_logloss: 0.0641141\tvalid_set's f1: 0.970233\u001B[32m [repeated 3x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=24256)\u001B[0m [10000]\tvalid_set's binary_logloss: 0.0655922\tvalid_set's f1: 0.970417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9675\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t135.31s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t45.56s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 3294.71s of the 5092.22s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9627\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.53s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.67s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3290.31s of the 5087.81s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9614\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.03s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.68s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 3286.40s of the 5083.90s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n",
      "\u001B[36m(_ray_fit pid=22184)\u001B[0m \tRan out of time, early stopping on iteration 8022.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9601\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2629.87s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.62s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 653.29s of the 2450.80s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9584\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.18s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 649.08s of the 2446.59s of remaining time.\n",
      "\u001B[36m(_ray_fit pid=24292)\u001B[0m \tRan out of time, early stopping on iteration 8191.\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9569\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.72s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.7s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 645.48s of the 2442.98s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.27%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9581\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t124.3s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.45s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 511.13s of the 2308.64s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.30%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9679\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t110.17s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t3.01s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 397.34s of the 2194.84s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.16%)\n",
      "\u001B[36m(_ray_fit pid=23048)\u001B[0m \tRan out of time, stopping training early. (Stopping on epoch 77)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9673\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t319.95s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.5s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 74.17s of the 1871.68s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.41%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m [1000]\tvalid_set's binary_logloss: 0.0566452\tvalid_set's f1: 0.962888\n",
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m [2000]\tvalid_set's binary_logloss: 0.0558062\tvalid_set's f1: 0.965337\u001B[32m [repeated 8x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m [3000]\tvalid_set's binary_logloss: 0.0568761\tvalid_set's f1: 0.967104\u001B[32m [repeated 7x across cluster]\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m \tRan out of time, early stopping on iteration 3371. Best iteration is:\n",
      "\u001B[36m(_ray_fit pid=16784)\u001B[0m \t[3366]\tvalid_set's binary_logloss: 0.0576004\tvalid_set's f1: 0.968094\n",
      "\u001B[36m(_ray_fit pid=23988)\u001B[0m \tRan out of time, stopping training early. (Stopping on epoch 78)\u001B[32m [repeated 7x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9685\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t61.51s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t107.11s\t = Validation runtime\n",
      "\u001B[36m(_ray_fit pid=13232)\u001B[0m \tRan out of time, early stopping on iteration 3303. Best iteration is:\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_ray_fit pid=13232)\u001B[0m \t[2885]\tvalid_set's binary_logloss: 0.0579046\tvalid_set's f1: 0.965852\u001B[32m [repeated 6x across cluster]\u001B[0m\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1790.53s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.3, 'LightGBMXT_BAG_L1': 0.15, 'LightGBM_BAG_L1': 0.15, 'RandomForestGini_BAG_L1': 0.15, 'RandomForestEntr_BAG_L1': 0.1, 'ExtraTreesGini_BAG_L1': 0.1, 'LightGBMLarge_BAG_L1': 0.05}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9702\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.04s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.02s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1785.43s of the 1785.42s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.58%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9704\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t7.73s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.59s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 1774.40s of the 1774.39s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.58%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9717\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t4.11s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.17s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 1767.09s of the 1767.08s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9725\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.66s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.69s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 1759.55s of the 1759.54s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9721\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.24s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.64s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1752.48s of the 1752.48s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.64%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9718\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t352.46s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.14s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 1396.89s of the 1396.88s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9726\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.71s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 1392.27s of the 1392.26s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.972\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.3s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.71s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 1388.08s of the 1388.07s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.86%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9725\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t133.67s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.06s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 1251.13s of the 1251.12s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.86%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9716\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t10.47s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.35s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1237.36s of the 1237.36s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.50%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9692\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t326.83s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.71s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 907.24s of the 907.23s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.80%)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9727\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t10.82s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.62s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 892.94s of remaining time.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'LightGBMLarge_BAG_L2': 0.75, 'ExtraTreesGini_BAG_L2': 0.167, 'RandomForestGini_BAG_L1': 0.083}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.9727\t = Validation score   (f1)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t9.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.02s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m AutoGluon training complete, total runtime = 4506.94s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 50.5 rows/s (10697 batch size)\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t8.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t8.61s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.53s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.67s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.03s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.68s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t601.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.18s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.72s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.7s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m No improvement since epoch 0: early stopping\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t49.28s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t3.65s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t161.67s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t8.96s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.3, 'LightGBMXT_BAG_L1': 0.15, 'LightGBM_BAG_L1': 0.15, 'RandomForestGini_BAG_L1': 0.15, 'RandomForestEntr_BAG_L1': 0.1, 'ExtraTreesGini_BAG_L1': 0.1, 'LightGBMLarge_BAG_L1': 0.05}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.04s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.74s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.44s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.66s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.69s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t5.24s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.64s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t68.34s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.71s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.73s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t2.3s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.71s\t = Validation runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m No improvement since epoch 0: early stopping\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t54.0s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: XGBoost_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t0.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t57.61s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: LightGBMLarge_BAG_L2_FULL ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t1.02s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tEnsemble Weights: {'LightGBMLarge_BAG_L2': 0.75, 'ExtraTreesGini_BAG_L2': 0.167, 'RandomForestGini_BAG_L1': 0.083}\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \t9.62s\t = Training   runtime\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Refit complete, total runtime = 1030.71s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tBase Threshold: 0.500\t| val: 0.9727\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tBest Threshold: 0.502\t| val: 0.9727\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Updating predictor.decision_threshold from 0.5 -> 0.502\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tPrediction probabilities of the positive class >0.502 will be predicted as the positive class (true). This can significantly impact metric scores.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m \tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m   warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n",
      "\u001B[36m(_dystack pid=14208)\u001B[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    NeuralNetFastAI_BAG_L2_FULL       0.972525   0.972518          f1        3.229756            NaN  905.557942                 0.159154                     NaN          53.996839            2       True         20\n",
      "1     ExtraTreesGini_BAG_L2_FULL       0.972362   0.972582          f1        3.209129            NaN  854.268252                 0.138527                1.725116           2.707150            2       True         18\n",
      "2           CatBoost_BAG_L2_FULL       0.972166   0.971815          f1        3.091247            NaN  919.904276                 0.020645                     NaN          68.343173            2       True         17\n",
      "3       WeightedEnsemble_L3_FULL       0.972166   0.972695          f1        3.272931            NaN  864.904662                 0.015032                     NaN           9.616641            3       True         24\n",
      "4     ExtraTreesEntr_BAG_L2_FULL       0.971797   0.971995          f1        3.216191            NaN  853.862036                 0.145589                1.706968           2.300934            2       True         19\n",
      "5            XGBoost_BAG_L2_FULL       0.971751   0.971565          f1        3.130604            NaN  852.179468                 0.060002                     NaN           0.618366            2       True         21\n",
      "6   RandomForestGini_BAG_L2_FULL       0.971578   0.972525          f1        3.218326            NaN  857.225250                 0.147724                1.690533           5.664147            2       True         15\n",
      "7         LightGBMXT_BAG_L2_FULL       0.971383   0.970391          f1        3.102315            NaN  852.296472                 0.031713                     NaN           0.735370            2       True         13\n",
      "8      LightGBMLarge_BAG_L2_FULL       0.971359   0.972662          f1        3.119372            NaN  852.580871                 0.048770                     NaN           1.019769            2       True         23\n",
      "9   RandomForestEntr_BAG_L2_FULL       0.971175   0.972124          f1        3.197128            NaN  856.801207                 0.126526                1.640430           5.240104            2       True         16\n",
      "10          LightGBM_BAG_L2_FULL       0.971129   0.971657          f1        3.095801            NaN  852.001240                 0.025199                     NaN           0.440137            2       True         14\n",
      "11          LightGBM_BAG_L1_FULL       0.970257   0.967521          f1        0.601471            NaN    8.614685                 0.601471                     NaN           8.614685            1       True          2\n",
      "12      WeightedEnsemble_L2_FULL       0.969453   0.970176          f1        2.587145            NaN  199.989231                 0.015180                     NaN           5.041221            2       True         12\n",
      "13     LightGBMLarge_BAG_L1_FULL       0.969246   0.968470          f1        0.679222            NaN    8.957639                 0.679222                     NaN           8.957639            1       True         11\n",
      "14    NeuralNetTorch_BAG_L2_FULL       0.968029   0.969195          f1        3.177464            NaN  909.172722                 0.106863                     NaN          57.611620            2       True         22\n",
      "15        LightGBMXT_BAG_L1_FULL       0.967884   0.967485          f1        0.698297            NaN    8.962849                 0.698297                     NaN           8.962849            1       True          1\n",
      "16    NeuralNetTorch_BAG_L1_FULL       0.966319   0.967343          f1        0.076677            NaN  161.669907                 0.076677                     NaN         161.669907            1       True         10\n",
      "17           XGBoost_BAG_L1_FULL       0.965738   0.967870          f1        0.133964            NaN    3.653278                 0.133964                     NaN           3.653278            1       True          9\n",
      "18  RandomForestGini_BAG_L1_FULL       0.963366   0.962727          f1        0.185536       1.666057    2.534030                 0.185536                1.666057           2.534030            1       True          3\n",
      "19  RandomForestEntr_BAG_L1_FULL       0.962336   0.961428          f1        0.160317       1.676287    2.032543                 0.160317                1.676287           2.032543            1       True          4\n",
      "20    ExtraTreesGini_BAG_L1_FULL       0.959258   0.958381          f1        0.170444       1.729316    2.176358                 0.170444                1.729316           2.176358            1       True          6\n",
      "21          CatBoost_BAG_L1_FULL       0.959064   0.960083          f1        0.058254            NaN  601.956029                 0.058254                     NaN         601.956029            1       True          5\n",
      "22    ExtraTreesEntr_BAG_L1_FULL       0.956837   0.956938          f1        0.154971       1.695606    1.718949                 0.154971                1.695606           1.718949            1       True          7\n",
      "23   NeuralNetFastAI_BAG_L1_FULL       0.924867   0.958110          f1        0.151449            NaN   49.284837                 0.151449                     NaN          49.284837            1       True          8\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t5556s\t = DyStack   runtime |\t16044s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 16044s\n",
      "AutoGluon will save models to \"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\"\n",
      "Train Data Rows:    96273\n",
      "Train Data Columns: 12\n",
      "Label Column:       label\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = true, class 0 = false\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (true) vs negative (false) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    6404.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 63.90 MB (1.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 12 | ['B1', 'B2', 'B3', 'B4', 'B5', ...]\n",
      "\t0.4s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.11 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 10692.98s of the 16043.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.22%)\n",
      "\t0.9697\t = Validation score   (f1)\n",
      "\t151.83s\t = Training   runtime\n",
      "\t112.99s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 10519.72s of the 15870.23s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.23%)\n",
      "\t0.9646\t = Validation score   (f1)\n",
      "\t121.24s\t = Training   runtime\n",
      "\t31.69s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 10385.39s of the 15735.90s of remaining time.\n",
      "\t0.9637\t = Validation score   (f1)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t1.79s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 10380.50s of the 15731.00s of remaining time.\n",
      "\t0.9621\t = Validation score   (f1)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 10376.01s of the 15726.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.26%)\n",
      "\t0.9627\t = Validation score   (f1)\n",
      "\t3625.98s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 6746.70s of the 12097.21s of remaining time.\n",
      "\t0.9594\t = Validation score   (f1)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t1.86s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 6742.37s of the 12092.88s of remaining time.\n",
      "\t0.957\t = Validation score   (f1)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t1.85s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 6738.21s of the 12088.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.34%)\n",
      "\t0.9584\t = Validation score   (f1)\n",
      "\t138.82s\t = Training   runtime\n",
      "\t1.56s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 6595.96s of the 11946.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.48%)\n",
      "\t0.9697\t = Validation score   (f1)\n",
      "\t170.55s\t = Training   runtime\n",
      "\t3.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 6421.18s of the 11771.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.19%)\n",
      "\t0.9711\t = Validation score   (f1)\n",
      "\t1260.35s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 5157.52s of the 10508.02s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.36%)\n",
      "\t0.9711\t = Validation score   (f1)\n",
      "\t103.18s\t = Training   runtime\n",
      "\t81.83s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1069.30s of the 10395.69s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.375, 'RandomForestGini_BAG_L1': 0.25, 'RandomForestEntr_BAG_L1': 0.125, 'XGBoost_BAG_L1': 0.125, 'LightGBMLarge_BAG_L1': 0.125}\n",
      "\t0.9726\t = Validation score   (f1)\n",
      "\t5.51s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 11 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 10390.14s of the 10390.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.68%)\n",
      "\t0.9726\t = Validation score   (f1)\n",
      "\t7.51s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 10379.29s of the 10379.27s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.69%)\n",
      "\t0.9737\t = Validation score   (f1)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 10372.82s of the 10372.80s of remaining time.\n",
      "\t0.9747\t = Validation score   (f1)\n",
      "\t6.55s\t = Training   runtime\n",
      "\t1.76s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 10364.32s of the 10364.30s of remaining time.\n",
      "\t0.9751\t = Validation score   (f1)\n",
      "\t5.83s\t = Training   runtime\n",
      "\t1.74s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 10356.57s of the 10356.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.75%)\n",
      "\t0.9739\t = Validation score   (f1)\n",
      "\t325.77s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 10027.55s of the 10027.54s of remaining time.\n",
      "\t0.9745\t = Validation score   (f1)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t1.87s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 10022.33s of the 10022.31s of remaining time.\n",
      "\t0.9746\t = Validation score   (f1)\n",
      "\t2.81s\t = Training   runtime\n",
      "\t1.88s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 10017.45s of the 10017.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.05%)\n",
      "\t0.9746\t = Validation score   (f1)\n",
      "\t144.44s\t = Training   runtime\n",
      "\t1.54s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 9869.67s of the 9869.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=1.09%)\n",
      "\t0.9742\t = Validation score   (f1)\n",
      "\t10.29s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 9855.90s of the 9855.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.62%)\n",
      "\t0.9715\t = Validation score   (f1)\n",
      "\t385.96s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 9466.54s of the 9466.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=2, gpus=0, memory=0.93%)\n",
      "\t0.9741\t = Validation score   (f1)\n",
      "\t5.13s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 1039.01s of the 9457.81s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestEntr_BAG_L2': 1.0}\n",
      "\t0.9751\t = Validation score   (f1)\n",
      "\t10.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6597.0s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 51.5 rows/s (12035 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t12.34s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t7.02s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.89s\t = Training   runtime\n",
      "\t1.79s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.5s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t808.88s\t = Training   runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.25s\t = Training   runtime\n",
      "\t1.86s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t2.1s\t = Training   runtime\n",
      "\t1.85s\t = Validation runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "No improvement since epoch 0: early stopping\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\t55.54s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t4.3s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t349.51s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t11.96s\t = Training   runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t5.83s\t = Training   runtime\n",
      "\t1.74s\t = Validation runtime\n",
      "Updated best model to \"RandomForestEntr_BAG_L2_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"RandomForestEntr_BAG_L2_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 1252.53s ... Best model: \"RandomForestEntr_BAG_L2_FULL\"\n",
      "Enabling decision threshold calibration (calibrate_decision_threshold='auto', metric is valid, problem_type is 'binary')\n",
      "Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\n",
      "Calibrating decision threshold via fine-grained search | Checking 38 thresholds...\n",
      "\tBase Threshold: 0.500\t| val: 0.9751\n",
      "\tBest Threshold: 0.559\t| val: 0.9752\n",
      "Updating predictor.decision_threshold from 0.5 -> 0.559\n",
      "\tThis will impact how prediction probabilities are converted to predictions in binary classification.\n",
      "\tPrediction probabilities of the positive class >0.559 will be predicted as the positive class (true). This can significantly impact metric scores.\n",
      "\tYou can update this value via `predictor.set_decision_threshold`.\n",
      "\tYou can calculate an optimal decision threshold on the validation data via `predictor.calibrate_decision_threshold()`.\n",
      "Deleting model LightGBMXT_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMXT_BAG_L1 will be removed.\n",
      "Deleting model LightGBM_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBM_BAG_L1 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestGini_BAG_L1 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestEntr_BAG_L1 will be removed.\n",
      "Deleting model CatBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\CatBoost_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesGini_BAG_L1 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesEntr_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetFastAI_BAG_L1 will be removed.\n",
      "Deleting model XGBoost_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\XGBoost_BAG_L1 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetTorch_BAG_L1 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L1. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMLarge_BAG_L1 will be removed.\n",
      "Deleting model WeightedEnsemble_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\WeightedEnsemble_L2 will be removed.\n",
      "Deleting model LightGBMXT_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMXT_BAG_L2 will be removed.\n",
      "Deleting model LightGBM_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBM_BAG_L2 will be removed.\n",
      "Deleting model RandomForestGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestGini_BAG_L2 will be removed.\n",
      "Deleting model RandomForestEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\RandomForestEntr_BAG_L2 will be removed.\n",
      "Deleting model CatBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\CatBoost_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesGini_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesGini_BAG_L2 will be removed.\n",
      "Deleting model ExtraTreesEntr_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\ExtraTreesEntr_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetFastAI_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetFastAI_BAG_L2 will be removed.\n",
      "Deleting model XGBoost_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\XGBoost_BAG_L2 will be removed.\n",
      "Deleting model NeuralNetTorch_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\NeuralNetTorch_BAG_L2 will be removed.\n",
      "Deleting model LightGBMLarge_BAG_L2. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\LightGBMLarge_BAG_L2 will be removed.\n",
      "Deleting model WeightedEnsemble_L3. All files under D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\\models\\WeightedEnsemble_L3 will be removed.\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\projects\\python\\Genome-Transition-Auto-MLl\\models\\ei\\combined\")\n",
      "D:\\projects\\python\\Genome-Transition-Auto-MLl\\.venv\\lib\\site-packages\\fastai\\learner.py:455: UserWarning: load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\n",
      "If you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\n",
      "  warn(\"load_learner` uses Python's insecure pickle module, which can execute malicious arbitrary code when loading. Only load files you trust.\\nIf you only need to load model weights and optimizer state, use the safe `Learner.load` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for combined:\n",
      "                           model  score_test score_val eval_metric  \\\n",
      "0   RandomForestEntr_BAG_L2_FULL    0.973865      None          f1   \n",
      "1      LightGBMLarge_BAG_L1_FULL    0.972153      None          f1   \n",
      "2            XGBoost_BAG_L1_FULL    0.971640      None          f1   \n",
      "3         LightGBMXT_BAG_L1_FULL    0.970114      None          f1   \n",
      "4           LightGBM_BAG_L1_FULL    0.969751      None          f1   \n",
      "5     NeuralNetTorch_BAG_L1_FULL    0.968885      None          f1   \n",
      "6   RandomForestGini_BAG_L1_FULL    0.964867      None          f1   \n",
      "7           CatBoost_BAG_L1_FULL    0.964247      None          f1   \n",
      "8   RandomForestEntr_BAG_L1_FULL    0.962901      None          f1   \n",
      "9     ExtraTreesGini_BAG_L1_FULL    0.956632      None          f1   \n",
      "10    ExtraTreesEntr_BAG_L1_FULL    0.954728      None          f1   \n",
      "11   NeuralNetFastAI_BAG_L1_FULL    0.915713      None          f1   \n",
      "\n",
      "    pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  \\\n",
      "0         9.866940            NaN  1265.118521                 0.185287   \n",
      "1         2.829647            NaN    11.962454                 2.829647   \n",
      "2         0.414227            NaN     4.297555                 0.414227   \n",
      "3         3.666034            NaN    12.339880                 3.666034   \n",
      "4         1.176956            NaN     7.021670                 1.176956   \n",
      "5         0.176993            NaN   349.512278                 0.176993   \n",
      "6         0.234456       1.786993     2.886807                 0.234456   \n",
      "7         0.117523            NaN   808.883289                 0.117523   \n",
      "8         0.202614       1.770829     2.495834                 0.202614   \n",
      "9         0.202652       1.864680     2.254139                 0.202652   \n",
      "10        0.202750       1.848873     2.099149                 0.202750   \n",
      "11        0.457801            NaN    55.538630                 0.457801   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                 1.737098           5.826838            2       True   \n",
      "1                      NaN          11.962454            1       True   \n",
      "2                      NaN           4.297555            1       True   \n",
      "3                      NaN          12.339880            1       True   \n",
      "4                      NaN           7.021670            1       True   \n",
      "5                      NaN         349.512278            1       True   \n",
      "6                 1.786993           2.886807            1       True   \n",
      "7                      NaN         808.883289            1       True   \n",
      "8                 1.770829           2.495834            1       True   \n",
      "9                 1.864680           2.254139            1       True   \n",
      "10                1.848873           2.099149            1       True   \n",
      "11                     NaN          55.538630            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0          12  \n",
      "1          11  \n",
      "2           9  \n",
      "3           1  \n",
      "4           2  \n",
      "5          10  \n",
      "6           3  \n",
      "7           5  \n",
      "8           4  \n",
      "9           6  \n",
      "10          7  \n",
      "11          8  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "true_file = \"../data/ie/data_ie.csv\"\n",
    "ie_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\"../data/ie/data_ie_random.csv\"),\n",
    "            pd.read_csv(\"../data/ie/data_ez_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ie/data_ei_true_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ie/data_ei_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ie/data_ze_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ie/data_test_false.csv\"),\n",
    "         ]\n",
    "    )\n",
    "}\n",
    "\n",
    "# For EI zone, our CSV files have columns B1 to B105.\n",
    "combined_data = prepare_data(ie_true_data, counter_files, max_index=105)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ie', 105, time_limit=21600)"
   ],
   "id": "16ef630f027dbbdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rue_file = \"../data/ez/data_ez.csv\"\n",
    "ez_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\"../data/ez/data_ez_random.csv\"),\n",
    "            pd.read_csv(\"../data/ez/data_ei_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ez/data_ie_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ez/data_ze_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ez/data_sample_combined.csv\"),\n",
    "         ]\n",
    "    )\n",
    "}\n",
    "\n",
    "# For EI zone, our CSV files have columns B1 to B550.\n",
    "combined_data = prepare_data(ez_true_data, counter_files, max_index=550)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ez', 550, time_limit=21600)"
   ],
   "id": "afd161db397f19e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "true_file = \"../data/ze/data_ze.csv\"\n",
    "ze_true_data = pd.read_csv(true_file)\n",
    "\n",
    "# List of counter example files for the EI zone\n",
    "counter_files = {\n",
    "    \"combined\": pd.concat(\n",
    "        [\n",
    "            pd.read_csv(\"../data/ze/data_ze_random.csv\"),\n",
    "            pd.read_csv(\"../data/ze/data_ez_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ze/data_ie_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ze/data_ei_counter_example.csv\"),\n",
    "            pd.read_csv(\"../data/ze/data_sample_combined.csv\"),\n",
    "         ]\n",
    "    )\n",
    "}\n",
    "\n",
    "# For EZ zone, our CSV files have columns B1 to B550.\n",
    "combined_data = prepare_data(ze_true_data, counter_files, max_index=550)\n",
    "\n",
    "# Dictionary to store the combined DataFrames and later model results\n",
    "generateModels(combined_data, 'ze', 550, time_limit=21600)"
   ],
   "id": "c9bf13ad02b3d5c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
